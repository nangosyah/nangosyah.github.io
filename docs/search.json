[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes\n\n\n\n\n\n\nTidyTuesday\n\n\n\n\n\n\n\n\n\nMay 6, 2025\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Diagnostic and Prognostic Potential of OCT Data in Multiple Sclerosis\n\n\n\n\n\n\nTutorials\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nMusic Dashboard: Stream & Play Music from Spotify\n\n\n\n\n\n\nDashboards\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the Tidy-models suite for Machine Learning\n\n\n\n\n\n\nMachine learning\n\n\nTidy-models\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nModel Based Clustering of High Dimensional Longitudinal Data\n\n\n\n\n\n\nLinear Mixed Models\n\n\nMixture Models\n\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "What I do\nTranslate data into decisions. I design decision support tools, apply statistical and Machine Learning methods, and lead delivery so insights become programs that work, informing strategy, business cases, and executive decisions.\nWhere I’ve worked\nImpact-focused settings across clean energy, micro-finance, socio-economic research, and start-ups working with cross-functional teams and securing multi-partner support to scale solutions.\nOutside of work, I enjoy Music, Football, Formula One and a lot of other sports.\nI’m always open to interesting conversations about where I can add value."
  },
  {
    "objectID": "posts/Music Dashboard/index.html",
    "href": "posts/Music Dashboard/index.html",
    "title": "Music Dashboard: Stream & Play Music from Spotify",
    "section": "",
    "text": "This interactive dashboard allows you to explore and listen to my curated playlist of music that I love.\n\n\n\nSong Selection: Choose from a personalized playlist of favorite tracks.\nEmbedded Player: Listen to selected songs directly from the dashboard.\n\nEnjoy !"
  },
  {
    "objectID": "posts/Music Dashboard/index.html#music-dashboard-stream-play-music-from-spotify",
    "href": "posts/Music Dashboard/index.html#music-dashboard-stream-play-music-from-spotify",
    "title": "Music Dashboard: Stream & Play Music from Spotify",
    "section": "",
    "text": "This interactive dashboard allows you to explore and listen to my curated playlist of music that I love.\n\n\n\nSong Selection: Choose from a personalized playlist of favorite tracks.\nEmbedded Player: Listen to selected songs directly from the dashboard.\n\nEnjoy !"
  },
  {
    "objectID": "posts/Thesis 2025/index.html",
    "href": "posts/Thesis 2025/index.html",
    "title": "Exploring the Diagnostic and Prognostic Potential of OCT Data in Multiple Sclerosis",
    "section": "",
    "text": "Machine learning (ML) is transforming Multiple Sclerosis (MS) diagnosis by leveraging Optical Coherence Tomography (OCT) data to assess retinal changes linked to neuro-degeneration. This study analyzed OCT data from 230 MS patients using ML models, including Random Forest (RF), Support Vector Machine (SVM), XGBoost, and k-Nearest Neighbors (KNN), to classify MS severity. RF emerged as the best performer, achieving F1-Scores of 0.74 (left eye) and 0.72 (right eye), with key retinal features such as the Superior and Temporal sectors, Central ILM-RPE, and asymmetry metrics identified as critical predictors. However, challenges remain, including measurement variability across OCT devices and segmentation inconsistencies, showing the need for standardization. Explore the modeling resources here"
  },
  {
    "objectID": "posts/Thesis 2025/index.html#exploring-the-diagnostic-and-prognostic-potential-of-oct-data-in-multiple-sclerosis",
    "href": "posts/Thesis 2025/index.html#exploring-the-diagnostic-and-prognostic-potential-of-oct-data-in-multiple-sclerosis",
    "title": "Exploring the Diagnostic and Prognostic Potential of OCT Data in Multiple Sclerosis",
    "section": "",
    "text": "Machine learning (ML) is transforming Multiple Sclerosis (MS) diagnosis by leveraging Optical Coherence Tomography (OCT) data to assess retinal changes linked to neuro-degeneration. This study analyzed OCT data from 230 MS patients using ML models, including Random Forest (RF), Support Vector Machine (SVM), XGBoost, and k-Nearest Neighbors (KNN), to classify MS severity. RF emerged as the best performer, achieving F1-Scores of 0.74 (left eye) and 0.72 (right eye), with key retinal features such as the Superior and Temporal sectors, Central ILM-RPE, and asymmetry metrics identified as critical predictors. However, challenges remain, including measurement variability across OCT devices and segmentation inconsistencies, showing the need for standardization. Explore the modeling resources here"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html",
    "href": "posts/Using Tidy Models for Machine Learning/index.html",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThe content of this tutorial is primarily based on the book “Tidy Modeling with R” by Max Kuhn and Julia Silge (2021). The analysis scheme also follows the approach outlined in the R Classification with Tidymodels tutorial.\n\n\nWe will use the Titanic dataset from Kaggle for our analysis, with the goal of building a model to predict which passengers survived the Titanic shipwreck. We will implement a classification workflow using the tidymodels package, demonstrating how workflows and recipes can be utilized for effective model building. Our research question is:\n“What sorts of people were more likely to survive?”\nTo address this question, we will consider factors such as the number of lifeboats, age, gender, and socio-economic class, based on the Titanic’s sinking on April 15, 1912. We will use classification methods to categorize passengers into those who survived and those who did not. Common classification techniques like logistic regression, random forests and K-nearest neighbors will be employed to optimize the solution with minimal error.\nFirst, we will load the necessary packages for the analysis:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bundle)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(readr)\nlibrary(stacks)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(ggmap)\nlibrary(visdat)\nlibrary(corrr)\nlibrary(ggsignif)\nlibrary(gt)\nlibrary(vip)\nlibrary(themis)\nlibrary(purrr)\nlibrary(keras)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(kknn)\nlibrary(reticulate)\n\nWe import the data for the analysis\n\nttest &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/test.csv\")\nttrain &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/train.csv\")\ntsub &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/gender_submission.csv\")\n\n\n\nTo gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)\n\n\n\n\nAfter applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy.\n\n\n\nTo enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))\n\n\n\n\nWe will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃\n\n\n\n\n\n\n\n\nIn machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n\n\n\nWe will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model.\n\n\n\n\nTo prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\n\nWe will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived) \n\n\n\n\nIn the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#format-data",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#format-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#missing-data",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#missing-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "After applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy."
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#create-variables",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#create-variables",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-overview",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-overview",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-splitting",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-splitting",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-exploration",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-exploration",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model."
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-preparation",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-preparation",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#validation-set",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#validation-set",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#model-building",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#model-building",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#compare-models",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#compare-models",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "Compare Models",
    "text": "Compare Models\nWe now extract the performance metrics from all the fitted models for comparison.\n\n# plot metrics\nggplot(mean_metrics, aes(x = model, y = estimate_value, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        axis.title.y = element_blank()) +\n  geom_text(aes(label = sprintf(\"%.2f\", estimate_value)), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, \n            size = 3) \n\n\n\n\n\n\n\n\nThe performance across the models is quite similar, with Random Forest performing slightly better. We will now evaluate the final model on the test set.\nTo accomplish this, the last_fit() function from the tidymodels package can be used. This function fits the model to the entire training dataset and evaluates it on the test set. You’ll need to provide the last_fit() function with the workflow object of the best model and the data split object (excluding the training data). This will allow us to obtain the performance metrics for the final model.\n\nlast_fit_rf &lt;- last_fit(rf_wflow, \n                        split = data_split,\n                        metrics = metric_set(\n                          recall, precision, f_meas, \n                          accuracy, kap,\n                          roc_auc, sens, spec)\n                        )\n\nTo display the performance metrics, we will use the collect_metrics() function as previously done.\n\nlast_fit_rf %&gt;%\n  collect_metrics()\n\n# A tibble: 8 × 4\n  .metric   .estimator .estimate .config             \n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 recall    binary         0.903 Preprocessor1_Model1\n2 precision binary         0.769 Preprocessor1_Model1\n3 f_meas    binary         0.830 Preprocessor1_Model1\n4 accuracy  binary         0.772 Preprocessor1_Model1\n5 kap       binary         0.492 Preprocessor1_Model1\n6 sens      binary         0.903 Preprocessor1_Model1\n7 spec      binary         0.562 Preprocessor1_Model1\n8 roc_auc   binary         0.789 Preprocessor1_Model1\n\n\nBased on our results we have a roc_auc of 0.7924757 which is generally considered a good performance although we could do better. This means our model has a high ability of finding true positive results than false positives.\nBased on our results, we should also examine variable importance to identify the key features influencing the classification.\n\nlast_fit_rf %&gt;%\n  pluck(\".workflow\", 1) %&gt;%  \n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 10)\n\n\n\n\n\n\n\n\nFrom the model we see the two most important predictors for our models is Sex and Age of the passenger.\nWe now take a look at the confusion matrix for the final model:\n\nlast_fit_rf %&gt;%\n  collect_predictions() |&gt;\n  conf_mat(Survived, .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nWe shall also create an ROC Curve for the final model:\n\nlast_fit_rf |&gt;\n  collect_predictions() |&gt;\n  roc_curve(Survived, .pred_0) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nGiven that the validation and test set performances are similar, we conclude that the Random Forest model with the selected hyperparameters is the best choice for predicting Survival on the Titanic.\n\nrf_predictions &lt;- last_fit_rf %&gt;%\n  collect_predictions()\n\n\n# Impute missing values in the test set\nttest &lt;- ttest %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(Age = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), Age, Age))\n\n# extract fitted workflow\nfinal_workflow &lt;- extract_workflow(last_fit_rf)\n\n# workflow for predictions\nrf_predictions &lt;- predict(final_workflow, ttest) %&gt;%\n  bind_cols(ttest %&gt;% select(PassengerId))\n\n# reorder table\nrf_predictions &lt;- rf_predictions %&gt;%\n  select(PassengerId, everything()) %&gt;% \n  rename(PassengerId = PassengerId, Survived = .pred_class)\n\nTo evaluate our model’s performance on the provided test set, we generated predictions using the final model and submitted them to Kaggle, achieving a Public Score of 0.77751.\n\nThis result, achieved with minimal feature engineering as demonstrated in the tutorial, indicates a somewhat good performance. However, there is potential for further improvement.\nBy incorporating additional feature engineering and exploring more advanced techniques, one could enhance the model’s accuracy.\nFurther feature extraction and refinement are recommended for those looking to achieve even better results for this model.\n\n# submission file\nwrite_csv(rf_predictions, \"submissionfile.csv\")\n\nTo conclude this tutorial, the data used in this project comes from the Kaggle Titanic - Machine Learning from Disaster competition. You can access and download the dataset by visiting the following Kaggle page."
  },
  {
    "objectID": "posts/Thesis/index.html",
    "href": "posts/Thesis/index.html",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Thesis/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "href": "posts/Thesis/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Fatal Cars/index.html",
    "href": "posts/Fatal Cars/index.html",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "",
    "text": "April 20th commonly known as “4/20” is an informal, globally recognized holiday celebrated by cannabis enthusiasts. In the United States in particular, the day features cultural events centered around marijuana use, public gatherings, education, and cannabis product trade fairs. The most active period typically falls between 4:20 PM and midnight, marking peak celebration hours.\nAs cannabis consumption becomes more mainstream especially in parts of the U.S., the Netherlands, and other regions where its use is legal or decriminalized public safety concerns have emerged. One of the most pressing questions is whether this unofficial holiday correlates with an increase in civil disobedience, particularly impaired driving and fatal car crashes in urban areas.\nPrevious studies have produced mixed results. For example, Harper and Palayew (2019) found no significant association between 4/20 and increased fatal crashes, while Staples and Redelmeier (2018) reported a noticeable spike in fatal traffic incidents on this date 12.\nAs part of the Tidy Tuesday data series, I explored this issue using several decades of U.S. traffic fatality data to gain deeper insight into the potential risks associated with 4/20 celebrations.\n\n\n\nDo fatal car crashes increase during the 4/20 celebration window (April 20th, 4:20 PM to 11:59 PM)?\nAre weekends more dangerous than weekdays when it comes to fatal traffic crashes?\nAre there noticeable patterns by season or day of the week in fatal crash data?\n\nUsing basic exploratory data analysis, visualization, and modeling, we shall run basic analysis on this data set. We start off by loading the packages and the data. The two tables (daily_accidents and daily_accidents_420) provide daily fatal crash counts, with the latter specifically identifying 4/20 dates.\n\nlibrary(tidytuesdayR)\nlibrary(GWalkR)\nlibrary(tidyverse)\n\nWe go ahead and extract the day of the week and month for each date to be able to analyze the temporal patterns in fatalities, latter on in the analysis.\n\n# load data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# create days\ndaily_accidents$days &lt;- weekdays(daily_accidents$date)\ndaily_accidents$months &lt;- factor(months(daily_accidents$date))\ndaily_accidents_420$days &lt;- weekdays(daily_accidents_420$date)\ndaily_accidents_420$months &lt;- factor(months(daily_accidents_420$date))\n\nNow that we have the day variable we would like to check for the general trend of fatalities on the different days of the week? to do this we calculate the mean number of daily fatalities for each weekday to identify patterns.\n\n# total accidents by day\nweekday_summary &lt;- daily_accidents_420 %&gt;%\n  group_by(days) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_fatalities))\n\nThe bar plot illustrates the average number of fatal car crashes for each day of the week. From the chart, we observe that weekend days, particularly Saturday and Sunday, along with Friday, tend to have the highest average fatalities. This pattern likely reflects increased recreational travel, social activities, and potentially higher rates of bad driving during these days.\n\n# average fatal car crashes\nggplot(weekday_summary, \n       aes(x = reorder(days, -avg_fatalities), y = avg_fatalities)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Weekday\", \n       y = \"Average Fatalities\", \n       title = \"Average Fatal Car Crashes by Weekday\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn contrast, the averages for Monday, Tuesday, Wednesday, and Thursday are noticeably lower and relatively similar to each other. While this difference is not directly explained by the dataset, one possible interpretation is that drivers may be more focused or cautious during the traditional workweek. However, it’s important to note that this is a speculative explanation and should not be taken as a conclusion drawn from the data itself, but rather as a contextual consideration that might warrant further investigation.\nTo understand how fatal car crashes have changed over time, we began by categorizing each day as either a weekday or weekend, and labeling April 20th (4/20) as a holiday. This allows us to structure the data in a way that enables meaningful comparisons between regular weekdays, weekends, and the 4/20 holiday.\n\n# indicators for weekend/weekday and 4/20\ndaily_accidents &lt;- daily_accidents %&gt;%\n  mutate(year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"))\n\ndaily_accidents_420 &lt;- daily_accidents_420 %&gt;%\n  mutate(days = weekdays(date),\n         year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"),\n         holiday = ifelse(e420 == TRUE, \"4/20\", \"Non-4/20\"))\ndaily_accidents$label &lt;- ifelse(daily_accidents$is_weekend, \"Weekend\", \"Weekday\")\ndaily_accidents_420$label &lt;- ifelse(daily_accidents_420$is_weekend, \"Weekend\", \"Weekday\")\n\nNext, we calculated the average number of fatalities per day, grouped by year and day type (weekday or weekend). We then visualized this data using a line plot to observe trends across the years.\n\n# average fatalities per day by year and label\nyear_fatal &lt;- daily_accidents_420 %&gt;%\n  group_by(year, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = 'drop')\n\n\nggplot(year_fatal, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(linewidth = 0.2) +\n  geom_point(size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  theme_test() +\n  scale_x_continuous(breaks = year_fatal$year, \n                     labels = year_fatal$year) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe resulting plot shows several key patterns:\n\nWeekends consistently show higher average fatality rates than weekdays. This is consistent with expectations, as weekends are often associated with more travel, leisure activities, and potentially bad driving.\nFrom 1992 to around 2005, both weekday and weekend fatalities were relatively high.\nThere was a decline in average daily fatalities from about 2005 to 2014, indicating a period of improved road safety.\nHowever, starting in 2014, the average fatalities began to increase again, and this trend continues in more recent years.\nImportantly, even during periods when overall fatalities declined, weekends still maintained higher averages compared to weekdays.\n\nWith this we can answer the question “Are weekends more dangerous than weekdays when it comes to fatal traffic crashes?” suggesting that weekends are consistently associated with a higher number of fatal crashes, regardless of the year.\nWe now shift our focus to a central question: Is April 20th (4/20) associated with a higher number of fatal car crashes compared to other days of the year? Given the cultural significance of 4/20 as a cannabis-focused celebration, it is reasonable to question whether increased substance use on this day contributes to increased crash rates.\nTo examine this, we grouped the dataset into two categories days that fall on 4/20 and all other calendar dates. Each of these groups was then subdivided based on whether the day was a weekday or a weekend. We calculated the average number of fatalities in each subgroup and visualized the results using a comparative bar chart.\n\n# Compare 4/20 vs non-4/20\nholiday_420 &lt;- daily_accidents_420 %&gt;%\n  group_by(holiday, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE),\n            count = n(),\n            .groups = \"drop\")\n\nThe plot reveals that fatal crash averages on 4/20 are actually lower than those on non-4/20 dates. This difference is especially pronounced on weekends, where one might expect the influence of recreational activity and bad driving to be higher. Contrary to popular assumptions, the data suggests that 4/20 does not experience a spike in fatal traffic incidents. In fact, the day tends to see slightly fewer fatalities on average.\n\n# plot\nggplot(holiday_420, aes(x = holiday, y = avg_fatalities, fill = label)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.6) +\n  geom_text(aes(label = round(avg_fatalities, 2)), \n            position = position_dodge(width = 0.7), \n            vjust = -0.5, size = 3.5) +\n  labs(\n    title = \"Average Fatalities on 4/20 vs Other Days\",\n    x = \"Day Category\",\n    y = \"Avg. Fatalities per Day\",\n    fill = \"Day Type\") +\n  theme_test()\n\n\n\n\n\n\n\n\nThese observations challenge the widely held perception that cannabis-related events on 4/20 lead to more dangerous roads. While the results do not eliminate the possibility of localized risks or isolated incidents, the broader national pattern does not support the idea that 4/20 is a particularly deadly day for drivers. To deepen our understanding of how April 20th compares to other days in terms of fatal crashes over time, we plotted the trend of average daily fatalities across all years, distinguishing between weekdays (in green), weekends (in blue), and April 20th specifically (in red). This allowed us to show how 4/20 fatalities track against broader national trends on regular days.\n\nyear_fatal_420 &lt;- daily_accidents_420 %&gt;%\n  filter(month(date) == 4 & day(date) == 20) %&gt;%\n  group_by(year = year(date)) %&gt;%\n  summarise(average_420_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = \"drop\")\n\nThe resulting plot offers a clear comparison. Across most years, the red dashed line representing the average number of fatalities on April 20th consistently falls below both weekday and weekend trends. This reinforces our earlier observation that 4/20 is not associated with a surge in fatal crashes, despite the day’s cultural reputation. In fact, while fatal crashes tend to spike on weekends and occasionally fluctuate over the years, 4/20’s average remains relatively stable and often lower than typical weekend averages.\n\nyear_fatal_with_420 &lt;- year_fatal %&gt;%\n  left_join(year_fatal_420, by = \"year\")\n\n# plot\nggplot(year_fatal_with_420, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(size = 0.2) +\n  geom_point(size = 1) +\n  geom_line(aes(x = year, y = average_420_fatalities), \n            color = \"red\", linetype = \"dashed\", size = 0.2) + \n  geom_point(aes(x = year, y = average_420_fatalities), \n             color = \"red\", size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  \n  theme_test() +\n  scale_color_manual(\n    values = c(\"Weekend\" = \"blue\", \"Weekday\" = \"green\", \"4/20 Average\" = \"red\"),\n    labels = c(\"Weekend\", \"Weekday\", \"4/20 Average\")) +\n  scale_x_continuous(breaks = year_fatal_with_420$year, \n                     labels = year_fatal_with_420$year) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\") +\n    geom_text(aes(x = max(year), y = max(average_420_fatalities), \n                  label = \"Average 4/20 Fatalities\"),\n              color = \"black\",\n              size = 4,\n              hjust = 1,\n              vjust = 6.5)\n\n\n\n\n\n\n\n\nThis strengthens our conclusion: April 20th does not exhibit a pattern of elevated fatal traffic incidents over the years when compared to the national averages for both weekdays and weekends. It directly answers our initial research question about whether 4/20 sees an increase in fatal crashes, it does not. Instead, the data suggest that public fears around cannabis-related driving fatalities on this day may be overstated, at least when looking at national trends over several decades.\nFollowing the exploratory trend analysis, we move into statistical modeling to test the impact of specific factors particularly weekends and the date April 20th (4/20) on the number of daily fatal traffic crashes. The goal here is to determine whether 4/20 is statistically associated with a significantly higher or lower risk of fatal crashes after accounting for other known influences like weekends, which we’ve already seen tend to have higher fatality rates.\nBefore applying any model, we first examine whether the number of daily fatalities (fatalities_count) meets assumptions required for common parametric tests. We start by checking normality across weekdays using QQ plots. The QQ plots show to some extent we can assume that the deviations from the normal distribution are not so off, which implies that daily fatalities are can be said to be normally distributed across days of the week.\n\nlibrary(lubridate)\nlibrary(car)\nlibrary(ggpubr)\n\n# check normality of each weekday group\nggqqplot(daily_accidents_420, \n         x = \"fatalities_count\", \n         facet.by = \"days\")\n\n\n\n\n\n\n\n\nWe would also want to complement this, we use Levene’s Test to check for homogeneity of variances across the weekday groups. Levene’s Test indicates that the assumption of equal variances is violated (significant p-value), meaning the variability in fatal crashes differs across days. This rules out traditional parametric methods.\n\n# Levene's Test for equal variances\nleveneTest(fatalities_count ~ days, data = daily_accidents_420)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    6  49.138 &lt; 2.2e-16 ***\n      9150                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the normality assumption was passed but equal variance assumptions are violated, we use the Kruskal-Wallis test, a non-parametric alternative to ANOVA that does not assume a specific distribution or equal variances.\n\nkruskal.test(fatalities_count ~ days, data = daily_accidents_420)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fatalities_count by days\nKruskal-Wallis chi-squared = 2742.3, df = 6, p-value &lt; 2.2e-16\n\n\nThe Kruskal-Wallis test result shows a highly significant p-value (&lt; 0.01), meaning that at least one day of the week has a statistically different distribution of fatal crashes. This further confirms the importance of day of week patterns in the cases of fatalities.\nNext, to evaluate the effect of weekends (is_weekend) and April 20th (e420) on daily fatal crash counts in a more controlled and quantifiable way, we turn to Poisson regression. Poisson models are commonly used when modeling count data, such as the number of fatal crashes per day.\n\n# Do Holidays or Weekends Have Higher Risk?\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(poissonreg)\n\npoisson &lt;- poisson_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(fatalities_count ~ is_weekend + e420, \n      data = daily_accidents_420)\npoisson\n\nparsnip model object\n\n\nCall:  stats::glm(formula = fatalities_count ~ is_weekend + e420, family = stats::poisson, \n    data = data)\n\nCoefficients:\n   (Intercept)  is_weekendTRUE        e420TRUE  \n        4.9190          0.1873         -0.9751  \n\nDegrees of Freedom: 9156 Total (i.e. Null);  9154 Residual\nNull Deviance:      71490 \nResidual Deviance: 59670    AIC: 121800\n\n\nThe Poisson model output shows:\nThe intercept represents the log expected count of fatalities on a typical weekday that is not 4/20.\n\nexp(4.92) ≈ 137, meaning we expect around 137 fatalities on an average non-weekend, non-4/20 day.\nThe weekend effect (is_weekendTRUE):\n\nexp(0.187) ≈ 1.206\nFatal crashes are 20.6% higher on weekends than on weekdays, a statistically significant difference.\n\nThe April 20th effect (e420TRUE):\n\nexp(-0.972) ≈ 0.377\nApril 20th is associated with a 62.3% decrease in fatal crashes, even after accounting for whether it’s a weekend. This is statistically significant and contradicts the common assumption that 4/20 is more dangerous.\n\n\nTo test whether our data violates this assumption, we check for over dispersion using the check_overdispersion() function.\n\n# check for overdispersion\nperformance::check_overdispersion(poisson$fit)\n\n# Overdispersion test\n\n       dispersion ratio =     6.529\n  Pearson's Chi-Squared = 59768.882\n                p-value =   &lt; 0.001\n\n\nThe result shows a dispersion ratio of 6.529 and a very small p-value (&lt; 0.001), indicating that the variance in fatality counts is much greater than the mean a classic sign of over dispersion. This invalidates the Poisson model, as it will underestimate standard errors and potentially lead to misleading significance tests. Given the over dispersion, we shift to a Negative Binomial Regression, which is more appropriate because it introduces an extra parameter to model the variance independently from the mean.\n\nlibrary(MASS)\n\nnb_model &lt;- glm.nb(fatalities_count ~ is_weekend + e420, data = daily_accidents_420)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = fatalities_count ~ is_weekend + e420, data = daily_accidents_420, \n    init.theta = 25.98766094, link = log)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     4.919055   0.002649 1857.28   &lt;2e-16 ***\nis_weekendTRUE  0.187202   0.004903   38.18   &lt;2e-16 ***\ne420TRUE       -0.972427   0.047733  -20.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(25.9877) family taken to be 1)\n\n    Null deviance: 11070.0  on 9156  degrees of freedom\nResidual deviance:  9221.3  on 9154  degrees of freedom\nAIC: 88473\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  25.988 \n          Std. Err.:  0.453 \n\n 2 x log-likelihood:  -88464.612 \n\n\nThe results from the Negative Binomial model are consistent with earlier findings but provide improved robustness due to better model assumptions. The model fit statistics show marked improvement. The AIC drops significantly from 121,800 in the Poisson model to 88,473 in the Negative Binomial model, and the residual deviance also decreases, indicating a much better fit to the data. These improvements validate the use of the Negative Binomial model for this analysis.\n\ntidy(nb_model, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     137.      0.00265    1857.  0         136.      138.   \n2 is_weekendTRUE    1.21    0.00490      38.2 0           1.19      1.22 \n3 e420TRUE          0.378   0.0477      -20.4 2.95e-92    0.345     0.415\n\n\n\n\n\nThis statistical modeling confirms and strengthens the earlier descriptive findings:\n\nWeekends are significantly more dangerous in terms of fatal crashes, as expected. They are associated with a 20.6% increase in fatal crashes (IRR ≈ 1.206).\nApril 20th does not increase fatal crash risk. On the contrary, the data consistently show that 4/20 is associated with a lower average number of fatal crashes, even when controlling for weekends and other factors. They are associated with a 62.2% decrease in fatalities (IRR ≈ 0.378).\n\nTherefore, the popular belief that 4/20 is a particularly dangerous day on U.S. roads is not supported by the data. In fact, this date shows a statistically significant reduction in fatalities, suggesting that public perception and media narratives about cannabis-related crashes on this day may be misleading.\nWhy might that be?\nSeveral factors could contribute to the lower fatality rate on 4/20, such as:\n\nIncreased public awareness and media attention\nGreater law enforcement presence\nFewer people driving (possibly choosing to stay home for cannabis-related events)\n\nThis analysis is a good reminder that data beats assumption, and that public discourse around events like 4/20 may not always align with the numbers."
  },
  {
    "objectID": "posts/Fatal Cars/index.html#introduction",
    "href": "posts/Fatal Cars/index.html#introduction",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "",
    "text": "April 20th commonly known as “4/20” is an informal, globally recognized holiday celebrated by cannabis enthusiasts. In the United States in particular, the day features cultural events centered around marijuana use, public gatherings, education, and cannabis product trade fairs. The most active period typically falls between 4:20 PM and midnight, marking peak celebration hours.\nAs cannabis consumption becomes more mainstream especially in parts of the U.S., the Netherlands, and other regions where its use is legal or decriminalized public safety concerns have emerged. One of the most pressing questions is whether this unofficial holiday correlates with an increase in civil disobedience, particularly impaired driving and fatal car crashes in urban areas.\nPrevious studies have produced mixed results. For example, Harper and Palayew (2019) found no significant association between 4/20 and increased fatal crashes, while Staples and Redelmeier (2018) reported a noticeable spike in fatal traffic incidents on this date 12.\nAs part of the Tidy Tuesday data series, I explored this issue using several decades of U.S. traffic fatality data to gain deeper insight into the potential risks associated with 4/20 celebrations.\n\n\n\nDo fatal car crashes increase during the 4/20 celebration window (April 20th, 4:20 PM to 11:59 PM)?\nAre weekends more dangerous than weekdays when it comes to fatal traffic crashes?\nAre there noticeable patterns by season or day of the week in fatal crash data?\n\nUsing basic exploratory data analysis, visualization, and modeling, we shall run basic analysis on this data set. We start off by loading the packages and the data. The two tables (daily_accidents and daily_accidents_420) provide daily fatal crash counts, with the latter specifically identifying 4/20 dates.\n\nlibrary(tidytuesdayR)\nlibrary(GWalkR)\nlibrary(tidyverse)\n\nWe go ahead and extract the day of the week and month for each date to be able to analyze the temporal patterns in fatalities, latter on in the analysis.\n\n# load data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# create days\ndaily_accidents$days &lt;- weekdays(daily_accidents$date)\ndaily_accidents$months &lt;- factor(months(daily_accidents$date))\ndaily_accidents_420$days &lt;- weekdays(daily_accidents_420$date)\ndaily_accidents_420$months &lt;- factor(months(daily_accidents_420$date))\n\nNow that we have the day variable we would like to check for the general trend of fatalities on the different days of the week? to do this we calculate the mean number of daily fatalities for each weekday to identify patterns.\n\n# total accidents by day\nweekday_summary &lt;- daily_accidents_420 %&gt;%\n  group_by(days) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_fatalities))\n\nThe bar plot illustrates the average number of fatal car crashes for each day of the week. From the chart, we observe that weekend days, particularly Saturday and Sunday, along with Friday, tend to have the highest average fatalities. This pattern likely reflects increased recreational travel, social activities, and potentially higher rates of bad driving during these days.\n\n# average fatal car crashes\nggplot(weekday_summary, \n       aes(x = reorder(days, -avg_fatalities), y = avg_fatalities)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Weekday\", \n       y = \"Average Fatalities\", \n       title = \"Average Fatal Car Crashes by Weekday\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn contrast, the averages for Monday, Tuesday, Wednesday, and Thursday are noticeably lower and relatively similar to each other. While this difference is not directly explained by the dataset, one possible interpretation is that drivers may be more focused or cautious during the traditional workweek. However, it’s important to note that this is a speculative explanation and should not be taken as a conclusion drawn from the data itself, but rather as a contextual consideration that might warrant further investigation.\nTo understand how fatal car crashes have changed over time, we began by categorizing each day as either a weekday or weekend, and labeling April 20th (4/20) as a holiday. This allows us to structure the data in a way that enables meaningful comparisons between regular weekdays, weekends, and the 4/20 holiday.\n\n# indicators for weekend/weekday and 4/20\ndaily_accidents &lt;- daily_accidents %&gt;%\n  mutate(year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"))\n\ndaily_accidents_420 &lt;- daily_accidents_420 %&gt;%\n  mutate(days = weekdays(date),\n         year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"),\n         holiday = ifelse(e420 == TRUE, \"4/20\", \"Non-4/20\"))\ndaily_accidents$label &lt;- ifelse(daily_accidents$is_weekend, \"Weekend\", \"Weekday\")\ndaily_accidents_420$label &lt;- ifelse(daily_accidents_420$is_weekend, \"Weekend\", \"Weekday\")\n\nNext, we calculated the average number of fatalities per day, grouped by year and day type (weekday or weekend). We then visualized this data using a line plot to observe trends across the years.\n\n# average fatalities per day by year and label\nyear_fatal &lt;- daily_accidents_420 %&gt;%\n  group_by(year, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = 'drop')\n\n\nggplot(year_fatal, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(linewidth = 0.2) +\n  geom_point(size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  theme_test() +\n  scale_x_continuous(breaks = year_fatal$year, \n                     labels = year_fatal$year) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe resulting plot shows several key patterns:\n\nWeekends consistently show higher average fatality rates than weekdays. This is consistent with expectations, as weekends are often associated with more travel, leisure activities, and potentially bad driving.\nFrom 1992 to around 2005, both weekday and weekend fatalities were relatively high.\nThere was a decline in average daily fatalities from about 2005 to 2014, indicating a period of improved road safety.\nHowever, starting in 2014, the average fatalities began to increase again, and this trend continues in more recent years.\nImportantly, even during periods when overall fatalities declined, weekends still maintained higher averages compared to weekdays.\n\nWith this we can answer the question “Are weekends more dangerous than weekdays when it comes to fatal traffic crashes?” suggesting that weekends are consistently associated with a higher number of fatal crashes, regardless of the year.\nWe now shift our focus to a central question: Is April 20th (4/20) associated with a higher number of fatal car crashes compared to other days of the year? Given the cultural significance of 4/20 as a cannabis-focused celebration, it is reasonable to question whether increased substance use on this day contributes to increased crash rates.\nTo examine this, we grouped the dataset into two categories days that fall on 4/20 and all other calendar dates. Each of these groups was then subdivided based on whether the day was a weekday or a weekend. We calculated the average number of fatalities in each subgroup and visualized the results using a comparative bar chart.\n\n# Compare 4/20 vs non-4/20\nholiday_420 &lt;- daily_accidents_420 %&gt;%\n  group_by(holiday, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE),\n            count = n(),\n            .groups = \"drop\")\n\nThe plot reveals that fatal crash averages on 4/20 are actually lower than those on non-4/20 dates. This difference is especially pronounced on weekends, where one might expect the influence of recreational activity and bad driving to be higher. Contrary to popular assumptions, the data suggests that 4/20 does not experience a spike in fatal traffic incidents. In fact, the day tends to see slightly fewer fatalities on average.\n\n# plot\nggplot(holiday_420, aes(x = holiday, y = avg_fatalities, fill = label)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.6) +\n  geom_text(aes(label = round(avg_fatalities, 2)), \n            position = position_dodge(width = 0.7), \n            vjust = -0.5, size = 3.5) +\n  labs(\n    title = \"Average Fatalities on 4/20 vs Other Days\",\n    x = \"Day Category\",\n    y = \"Avg. Fatalities per Day\",\n    fill = \"Day Type\") +\n  theme_test()\n\n\n\n\n\n\n\n\nThese observations challenge the widely held perception that cannabis-related events on 4/20 lead to more dangerous roads. While the results do not eliminate the possibility of localized risks or isolated incidents, the broader national pattern does not support the idea that 4/20 is a particularly deadly day for drivers. To deepen our understanding of how April 20th compares to other days in terms of fatal crashes over time, we plotted the trend of average daily fatalities across all years, distinguishing between weekdays (in green), weekends (in blue), and April 20th specifically (in red). This allowed us to show how 4/20 fatalities track against broader national trends on regular days.\n\nyear_fatal_420 &lt;- daily_accidents_420 %&gt;%\n  filter(month(date) == 4 & day(date) == 20) %&gt;%\n  group_by(year = year(date)) %&gt;%\n  summarise(average_420_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = \"drop\")\n\nThe resulting plot offers a clear comparison. Across most years, the red dashed line representing the average number of fatalities on April 20th consistently falls below both weekday and weekend trends. This reinforces our earlier observation that 4/20 is not associated with a surge in fatal crashes, despite the day’s cultural reputation. In fact, while fatal crashes tend to spike on weekends and occasionally fluctuate over the years, 4/20’s average remains relatively stable and often lower than typical weekend averages.\n\nyear_fatal_with_420 &lt;- year_fatal %&gt;%\n  left_join(year_fatal_420, by = \"year\")\n\n# plot\nggplot(year_fatal_with_420, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(size = 0.2) +\n  geom_point(size = 1) +\n  geom_line(aes(x = year, y = average_420_fatalities), \n            color = \"red\", linetype = \"dashed\", size = 0.2) + \n  geom_point(aes(x = year, y = average_420_fatalities), \n             color = \"red\", size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  \n  theme_test() +\n  scale_color_manual(\n    values = c(\"Weekend\" = \"blue\", \"Weekday\" = \"green\", \"4/20 Average\" = \"red\"),\n    labels = c(\"Weekend\", \"Weekday\", \"4/20 Average\")) +\n  scale_x_continuous(breaks = year_fatal_with_420$year, \n                     labels = year_fatal_with_420$year) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\") +\n    geom_text(aes(x = max(year), y = max(average_420_fatalities), \n                  label = \"Average 4/20 Fatalities\"),\n              color = \"black\",\n              size = 4,\n              hjust = 1,\n              vjust = 6.5)\n\n\n\n\n\n\n\n\nThis strengthens our conclusion: April 20th does not exhibit a pattern of elevated fatal traffic incidents over the years when compared to the national averages for both weekdays and weekends. It directly answers our initial research question about whether 4/20 sees an increase in fatal crashes, it does not. Instead, the data suggest that public fears around cannabis-related driving fatalities on this day may be overstated, at least when looking at national trends over several decades.\nFollowing the exploratory trend analysis, we move into statistical modeling to test the impact of specific factors particularly weekends and the date April 20th (4/20) on the number of daily fatal traffic crashes. The goal here is to determine whether 4/20 is statistically associated with a significantly higher or lower risk of fatal crashes after accounting for other known influences like weekends, which we’ve already seen tend to have higher fatality rates.\nBefore applying any model, we first examine whether the number of daily fatalities (fatalities_count) meets assumptions required for common parametric tests. We start by checking normality across weekdays using QQ plots. The QQ plots show to some extent we can assume that the deviations from the normal distribution are not so off, which implies that daily fatalities are can be said to be normally distributed across days of the week.\n\nlibrary(lubridate)\nlibrary(car)\nlibrary(ggpubr)\n\n# check normality of each weekday group\nggqqplot(daily_accidents_420, \n         x = \"fatalities_count\", \n         facet.by = \"days\")\n\n\n\n\n\n\n\n\nWe would also want to complement this, we use Levene’s Test to check for homogeneity of variances across the weekday groups. Levene’s Test indicates that the assumption of equal variances is violated (significant p-value), meaning the variability in fatal crashes differs across days. This rules out traditional parametric methods.\n\n# Levene's Test for equal variances\nleveneTest(fatalities_count ~ days, data = daily_accidents_420)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    6  49.138 &lt; 2.2e-16 ***\n      9150                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the normality assumption was passed but equal variance assumptions are violated, we use the Kruskal-Wallis test, a non-parametric alternative to ANOVA that does not assume a specific distribution or equal variances.\n\nkruskal.test(fatalities_count ~ days, data = daily_accidents_420)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fatalities_count by days\nKruskal-Wallis chi-squared = 2742.3, df = 6, p-value &lt; 2.2e-16\n\n\nThe Kruskal-Wallis test result shows a highly significant p-value (&lt; 0.01), meaning that at least one day of the week has a statistically different distribution of fatal crashes. This further confirms the importance of day of week patterns in the cases of fatalities.\nNext, to evaluate the effect of weekends (is_weekend) and April 20th (e420) on daily fatal crash counts in a more controlled and quantifiable way, we turn to Poisson regression. Poisson models are commonly used when modeling count data, such as the number of fatal crashes per day.\n\n# Do Holidays or Weekends Have Higher Risk?\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(poissonreg)\n\npoisson &lt;- poisson_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(fatalities_count ~ is_weekend + e420, \n      data = daily_accidents_420)\npoisson\n\nparsnip model object\n\n\nCall:  stats::glm(formula = fatalities_count ~ is_weekend + e420, family = stats::poisson, \n    data = data)\n\nCoefficients:\n   (Intercept)  is_weekendTRUE        e420TRUE  \n        4.9190          0.1873         -0.9751  \n\nDegrees of Freedom: 9156 Total (i.e. Null);  9154 Residual\nNull Deviance:      71490 \nResidual Deviance: 59670    AIC: 121800\n\n\nThe Poisson model output shows:\nThe intercept represents the log expected count of fatalities on a typical weekday that is not 4/20.\n\nexp(4.92) ≈ 137, meaning we expect around 137 fatalities on an average non-weekend, non-4/20 day.\nThe weekend effect (is_weekendTRUE):\n\nexp(0.187) ≈ 1.206\nFatal crashes are 20.6% higher on weekends than on weekdays, a statistically significant difference.\n\nThe April 20th effect (e420TRUE):\n\nexp(-0.972) ≈ 0.377\nApril 20th is associated with a 62.3% decrease in fatal crashes, even after accounting for whether it’s a weekend. This is statistically significant and contradicts the common assumption that 4/20 is more dangerous.\n\n\nTo test whether our data violates this assumption, we check for over dispersion using the check_overdispersion() function.\n\n# check for overdispersion\nperformance::check_overdispersion(poisson$fit)\n\n# Overdispersion test\n\n       dispersion ratio =     6.529\n  Pearson's Chi-Squared = 59768.882\n                p-value =   &lt; 0.001\n\n\nThe result shows a dispersion ratio of 6.529 and a very small p-value (&lt; 0.001), indicating that the variance in fatality counts is much greater than the mean a classic sign of over dispersion. This invalidates the Poisson model, as it will underestimate standard errors and potentially lead to misleading significance tests. Given the over dispersion, we shift to a Negative Binomial Regression, which is more appropriate because it introduces an extra parameter to model the variance independently from the mean.\n\nlibrary(MASS)\n\nnb_model &lt;- glm.nb(fatalities_count ~ is_weekend + e420, data = daily_accidents_420)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = fatalities_count ~ is_weekend + e420, data = daily_accidents_420, \n    init.theta = 25.98766094, link = log)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     4.919055   0.002649 1857.28   &lt;2e-16 ***\nis_weekendTRUE  0.187202   0.004903   38.18   &lt;2e-16 ***\ne420TRUE       -0.972427   0.047733  -20.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(25.9877) family taken to be 1)\n\n    Null deviance: 11070.0  on 9156  degrees of freedom\nResidual deviance:  9221.3  on 9154  degrees of freedom\nAIC: 88473\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  25.988 \n          Std. Err.:  0.453 \n\n 2 x log-likelihood:  -88464.612 \n\n\nThe results from the Negative Binomial model are consistent with earlier findings but provide improved robustness due to better model assumptions. The model fit statistics show marked improvement. The AIC drops significantly from 121,800 in the Poisson model to 88,473 in the Negative Binomial model, and the residual deviance also decreases, indicating a much better fit to the data. These improvements validate the use of the Negative Binomial model for this analysis.\n\ntidy(nb_model, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     137.      0.00265    1857.  0         136.      138.   \n2 is_weekendTRUE    1.21    0.00490      38.2 0           1.19      1.22 \n3 e420TRUE          0.378   0.0477      -20.4 2.95e-92    0.345     0.415\n\n\n\n\n\nThis statistical modeling confirms and strengthens the earlier descriptive findings:\n\nWeekends are significantly more dangerous in terms of fatal crashes, as expected. They are associated with a 20.6% increase in fatal crashes (IRR ≈ 1.206).\nApril 20th does not increase fatal crash risk. On the contrary, the data consistently show that 4/20 is associated with a lower average number of fatal crashes, even when controlling for weekends and other factors. They are associated with a 62.2% decrease in fatalities (IRR ≈ 0.378).\n\nTherefore, the popular belief that 4/20 is a particularly dangerous day on U.S. roads is not supported by the data. In fact, this date shows a statistically significant reduction in fatalities, suggesting that public perception and media narratives about cannabis-related crashes on this day may be misleading.\nWhy might that be?\nSeveral factors could contribute to the lower fatality rate on 4/20, such as:\n\nIncreased public awareness and media attention\nGreater law enforcement presence\nFewer people driving (possibly choosing to stay home for cannabis-related events)\n\nThis analysis is a good reminder that data beats assumption, and that public discourse around events like 4/20 may not always align with the numbers."
  },
  {
    "objectID": "posts/Fatal Cars/index.html#footnotes",
    "href": "posts/Fatal Cars/index.html#footnotes",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHarper S, Palayew A. The annual cannabis holiday and fatal traffic crashes. BMJ Injury Prevention. Published Online First: 29 January 2019. https://doi.org/10.1136/injuryprev-2018-043068. Manuscript and data/code↩︎\nStaples JA, Redelmeier DA. The April 20 cannabis celebration and fatal traffic crashes in the United States. JAMA Intern Med. 2018;178(4):569–572. https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2672751↩︎"
  },
  {
    "objectID": "just.html",
    "href": "just.html",
    "title": "About",
    "section": "",
    "text": "&lt;div class=\"row align-items-center gy-4\"&gt;\n  &lt;div class=\"col-12 col-lg-4\"&gt;\n    &lt;img src=\"tompassport.jpg\" alt=\"Tom Nangosyah\" class=\"hero-img rounded-3 shadow-soft\"&gt;\n  &lt;/div&gt;\n  &lt;div class=\"col-12 col-lg-8\"&gt;\n    &lt;h1 class=\"display-5 mb-2\"&gt;Data → Decisions → Delivery&lt;/h1&gt;\n    &lt;p class=\"lead text-secondary-emphasis mb-4\"&gt;\n      I blend analytics, research, and program delivery to turn complex ideas into evidence-based results across clean energy, micro-finance, and social impact.\n      Currently exploring roles in &lt;strong&gt;health data & bioinformatics&lt;/strong&gt;, &lt;strong&gt;development project delivery&lt;/strong&gt;, and &lt;strong&gt;innovation pilots&lt;/strong&gt;.\n    &lt;/p&gt;\n    &lt;div class=\"d-flex flex-wrap gap-2\"&gt;\n      &lt;a class=\"btn btn-dark btn-lg\" href=\"mailto:nangosyahtom@gmail.com\"&gt;Let’s talk&lt;/a&gt;\n      &lt;a class=\"btn btn-outline-dark btn-lg\" href=\"https://nangosyah.github.io/portfolio.html\"View portfolio&lt;/a&gt;\n      &lt;a class=\"btn btn-outline-secondary btn-lg\" href=\"Nangosyah_CV.pdf\"&gt;Download CV&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"row gy-4\"&gt;\n  &lt;div class=\"col-12 col-lg-8\"&gt;\n    &lt;h2 class=\"section-title\"&gt;Snapshot&lt;/h2&gt;\n    &lt;p class=\"mb-3\"&gt;\n      Seven years bridging field reality and data rigor: designing pilots, building unit-economics, and aligning donors, partners, and communities.\n      Comfortable in code, in workshops, and in the field. I care about outcomes, not outputs.\n    &lt;/p&gt;\n\n    &lt;div class=\"row g-3\"&gt;\n      &lt;div class=\"col-md-6\"&gt;\n        &lt;div class=\"card lift\"&gt;\n          &lt;div class=\"card-body\"&gt;\n            &lt;h3 class=\"h5 mb-2\"&gt;What I do&lt;/h3&gt;\n            &lt;ul class=\"list-unstyled mb-0\"&gt;\n              &lt;li&gt;• Translate raw data into clear, defensible decisions&lt;/li&gt;\n              &lt;li&gt;• Design & run pilots; measure what matters&lt;/li&gt;\n              &lt;li&gt;• Build stakeholder alignment & funding cases&lt;/li&gt;\n              &lt;li&gt;• Ship end-to-end programs with cross-functional teams&lt;/li&gt;\n            &lt;/ul&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class=\"col-md-6\"&gt;\n        &lt;div class=\"card lift\"&gt;\n          &lt;div class=\"card-body\"&gt;\n            &lt;h3 class=\"h5 mb-2\"&gt;Focus areas&lt;/h3&gt;\n            &lt;div class=\"tag-grid\"&gt;\n              &lt;span class=\"tag\"&gt;Health data&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Bioinformatics&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Clean cooking&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Energy access&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Micro-finance&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Innovation pilots&lt;/span&gt;\n              &lt;span class=\"tag\"&gt;Strategy & transformation&lt;/span&gt;\n            &lt;/div&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;!-- Callout --&gt;\n    &lt;div class=\"callout mt-4\"&gt;\n      &lt;p class=\"mb-0\"&gt;\n        &lt;strong&gt;Open to conversations&lt;/strong&gt; on where I can add the most value—advisory, analysis, or delivery.\n      &lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"col-12 col-lg-4\"&gt;\n    &lt;h2 class=\"section-title\"&gt;By the numbers&lt;/h2&gt;\n    &lt;div class=\"stats\"&gt;\n      &lt;div class=\"stat\"&gt;\n        &lt;div class=\"stat-value\"&gt;4,000+&lt;/div&gt;\n        &lt;div class=\"stat-label\"&gt;surveys analyzed&lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class=\"stat\"&gt;\n        &lt;div class=\"stat-value\"&gt;250+&lt;/div&gt;\n        &lt;div class=\"stat-label\"&gt;SMEs financed&lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class=\"stat\"&gt;\n        &lt;div class=\"stat-value\"&gt;8+&lt;/div&gt;\n        &lt;div class=\"stat-label\"&gt;donor/partner orgs&lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class=\"stat\"&gt;\n        &lt;div class=\"stat-value\"&gt;7 yrs&lt;/div&gt;\n        &lt;div class=\"stat-label\"&gt;analytics & delivery&lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"card mt-4 soft-border\"&gt;\n      &lt;div class=\"card-body\"&gt;\n        &lt;h3 class=\"h6 text-uppercase text-muted mb-2\"&gt;Toolbox&lt;/h3&gt;\n        &lt;div class=\"tag-grid\"&gt;\n          &lt;span class=\"tag\"&gt;Python&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;R&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;SQL / Access&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;Statistical modeling&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;ML & prediction&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;Dashboards&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;KPI pipelines&lt;/span&gt;\n          &lt;span class=\"tag\"&gt;Unit economics&lt;/span&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n\n\n\n&lt;h2 class=\"section-title\"&gt;Selected highlights&lt;/h2&gt;\n&lt;div class=\"row row-cols-1 row-cols-md-3 g-4\"&gt;\n  &lt;div class=\"col\"&gt;\n    &lt;a class=\"card card-link lift h-100\" href=\"https://nangosyah.github.io/portfolio.html#clean-cooking\"\n      &lt;div class=\"card-body\"&gt;\n        &lt;h3 class=\"h5 mb-2\"&gt;Clean cooking pilot&lt;/h3&gt;\n        &lt;p class=\"mb-0\"&gt;Mapped ethanol supply chains, built bottom-up unit economics, and produced investor-ready materials to launch in Uganda.&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n  &lt;div class=\"col\"&gt;\n    &lt;a class=\"card card-link lift h-100\" href=\"https://nangosyah.github.io/portfolio.html#utilities\"\n      &lt;div class=\"card-body\"&gt;\n        &lt;h3 class=\"h5 mb-2\"&gt;Utilities 2.0 (Twaake)&lt;/h3&gt;\n        &lt;p class=\"mb-0\"&gt;Aligned productive-use finance with grid/mini-grid ops; defined KPIs and reporting cadence across partners.&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n  &lt;div class=\"col\"&gt;\n    &lt;a class=\"card card-link lift h-100\" href=\"https://nangosyah.github.io/portfolio.html#analytics\"\n      &lt;div class=\"card-body\"&gt;\n        &lt;h3 class=\"h5 mb-2\"&gt;Analytics at scale&lt;/h3&gt;\n        &lt;p class=\"mb-0\"&gt;Built pipelines & dashboards translating operational and financial data into executive decisions.&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n\n\n\n\n&lt;div&gt;\n  &lt;h3 class=\"mb-1\"&gt;Let’s build something useful.&lt;/h3&gt;\n  &lt;p class=\"mb-0 text-secondary\"&gt;Advisory, analysis, or end-to-end delivery—happy to dive in.&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"d-flex gap-2\"&gt;\n  &lt;a class=\"btn btn-dark btn-lg\" href=\"mailto:nangosyahtom@gmail.com\"&gt;Email me&lt;/a&gt;\n  &lt;a class=\"btn btn-outline-dark btn-lg\" href=\"https://www.linkedin.com/in/tom-wellard-nangosyah-044435a1/\"Connect&lt;/a&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThe content of this tutorial is primarily based on the book “Tidy Modeling with R” by Max Kuhn and Julia Silge (2021). The analysis scheme also follows the approach outlined in the R Classification with Tidymodels tutorial.\n\n\nWe will use the Titanic dataset from Kaggle for our analysis, with the goal of building a model to predict which passengers survived the Titanic shipwreck. We will implement a classification workflow using the tidymodels package, demonstrating how workflows and recipes can be utilized for effective model building. Our research question is:\n“What sorts of people were more likely to survive?”\nTo address this question, we will consider factors such as the number of lifeboats, age, gender, and socio-economic class, based on the Titanic’s sinking on April 15, 1912. We will use classification methods to categorize passengers into those who survived and those who did not. Common classification techniques like logistic regression, random forests and K-nearest neighbors will be employed to optimize the solution with minimal error.\nFirst, we will load the necessary packages for the analysis:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bundle)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(readr)\nlibrary(stacks)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(ggmap)\nlibrary(visdat)\nlibrary(corrr)\nlibrary(ggsignif)\nlibrary(gt)\nlibrary(vip)\nlibrary(themis)\nlibrary(purrr)\nlibrary(keras)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(kknn)\nlibrary(reticulate)\n\nWe import the data for the analysis\n\nttest &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/test.csv\")\nttrain &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/train.csv\")\ntsub &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/gender_submission.csv\")\n\n\n\nTo gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)\n\n\n\n\nAfter applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy.\n\n\n\nTo enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))\n\n\n\n\nWe will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃\n\n\n\n\n\n\n\n\nIn machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n\n\n\nWe will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model.\n\n\n\n\nTo prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\n\nWe will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived) \n\n\n\n\nIn the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#format-data",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#format-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#missing-data",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#missing-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "After applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy."
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#create-variables",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#create-variables",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-overview",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-overview",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-splitting",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-splitting",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-exploration",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-exploration",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model."
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-preparation",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#data-preparation",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#validation-set",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#validation-set",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived)"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#model-building",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#model-building",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Tutorial - Tidy Models for Machine Learning/index.html#compare-models",
    "href": "posts/Tutorial - Tidy Models for Machine Learning/index.html#compare-models",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "Compare Models",
    "text": "Compare Models\nWe now extract the performance metrics from all the fitted models for comparison.\n\n# plot metrics\nggplot(mean_metrics, aes(x = model, y = estimate_value, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        axis.title.y = element_blank()) +\n  geom_text(aes(label = sprintf(\"%.2f\", estimate_value)), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, \n            size = 3) \n\n\n\n\n\n\n\n\nThe performance across the models is quite similar, with Random Forest performing slightly better. We will now evaluate the final model on the test set.\nTo accomplish this, the last_fit() function from the tidymodels package can be used. This function fits the model to the entire training dataset and evaluates it on the test set. You’ll need to provide the last_fit() function with the workflow object of the best model and the data split object (excluding the training data). This will allow us to obtain the performance metrics for the final model.\n\nlast_fit_rf &lt;- last_fit(rf_wflow, \n                        split = data_split,\n                        metrics = metric_set(\n                          recall, precision, f_meas, \n                          accuracy, kap,\n                          roc_auc, sens, spec)\n                        )\n\nTo display the performance metrics, we will use the collect_metrics() function as previously done.\n\nlast_fit_rf %&gt;%\n  collect_metrics()\n\n# A tibble: 8 × 4\n  .metric   .estimator .estimate .config             \n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 recall    binary         0.903 Preprocessor1_Model1\n2 precision binary         0.769 Preprocessor1_Model1\n3 f_meas    binary         0.830 Preprocessor1_Model1\n4 accuracy  binary         0.772 Preprocessor1_Model1\n5 kap       binary         0.492 Preprocessor1_Model1\n6 sens      binary         0.903 Preprocessor1_Model1\n7 spec      binary         0.562 Preprocessor1_Model1\n8 roc_auc   binary         0.789 Preprocessor1_Model1\n\n\nBased on our results we have a roc_auc of 0.7924757 which is generally considered a good performance although we could do better. This means our model has a high ability of finding true positive results than false positives.\nBased on our results, we should also examine variable importance to identify the key features influencing the classification.\n\nlast_fit_rf %&gt;%\n  pluck(\".workflow\", 1) %&gt;%  \n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 10)\n\n\n\n\n\n\n\n\nFrom the model we see the two most important predictors for our models is Sex and Age of the passenger.\nWe now take a look at the confusion matrix for the final model:\n\nlast_fit_rf %&gt;%\n  collect_predictions() |&gt;\n  conf_mat(Survived, .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nWe shall also create an ROC Curve for the final model:\n\nlast_fit_rf |&gt;\n  collect_predictions() |&gt;\n  roc_curve(Survived, .pred_0) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nGiven that the validation and test set performances are similar, we conclude that the Random Forest model with the selected hyperparameters is the best choice for predicting Survival on the Titanic.\n\nrf_predictions &lt;- last_fit_rf %&gt;%\n  collect_predictions()\n\n\n# Impute missing values in the test set\nttest &lt;- ttest %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(Age = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), Age, Age))\n\n# extract fitted workflow\nfinal_workflow &lt;- extract_workflow(last_fit_rf)\n\n# workflow for predictions\nrf_predictions &lt;- predict(final_workflow, ttest) %&gt;%\n  bind_cols(ttest %&gt;% select(PassengerId))\n\n# reorder table\nrf_predictions &lt;- rf_predictions %&gt;%\n  select(PassengerId, everything()) %&gt;% \n  rename(PassengerId = PassengerId, Survived = .pred_class)\n\nTo evaluate our model’s performance on the provided test set, we generated predictions using the final model and submitted them to Kaggle, achieving a Public Score of 0.77751.\n\nThis result, achieved with minimal feature engineering as demonstrated in the tutorial, indicates a somewhat good performance. However, there is potential for further improvement.\nBy incorporating additional feature engineering and exploring more advanced techniques, one could enhance the model’s accuracy.\nFurther feature extraction and refinement are recommended for those looking to achieve even better results for this model.\n\n# submission file\nwrite_csv(rf_predictions, \"submissionfile.csv\")\n\nTo conclude this tutorial, the data used in this project comes from the Kaggle Titanic - Machine Learning from Disaster competition. You can access and download the dataset by visiting the following Kaggle page."
  },
  {
    "objectID": "posts/Master Thesis/index.html",
    "href": "posts/Master Thesis/index.html",
    "title": "Exploring the Diagnostic and Prognostic Potential of OCT Data in Multiple Sclerosis",
    "section": "",
    "text": "Machine learning (ML) is transforming Multiple Sclerosis (MS) diagnosis by leveraging Optical Coherence Tomography (OCT) data to assess retinal changes linked to neuro-degeneration. This study analyzed OCT data from 230 MS patients using ML models, including Random Forest (RF), Support Vector Machine (SVM), XGBoost, and k-Nearest Neighbors (KNN), to classify MS severity. RF emerged as the best performer, achieving F1-Scores of 0.74 (left eye) and 0.72 (right eye), with key retinal features such as the Superior and Temporal sectors, Central ILM-RPE, and asymmetry metrics identified as critical predictors. However, challenges remain, including measurement variability across OCT devices and segmentation inconsistencies, showing the need for standardization. Explore the modeling resources here"
  },
  {
    "objectID": "posts/Master Thesis/index.html#exploring-the-diagnostic-and-prognostic-potential-of-oct-data-in-multiple-sclerosis",
    "href": "posts/Master Thesis/index.html#exploring-the-diagnostic-and-prognostic-potential-of-oct-data-in-multiple-sclerosis",
    "title": "Exploring the Diagnostic and Prognostic Potential of OCT Data in Multiple Sclerosis",
    "section": "",
    "text": "Machine learning (ML) is transforming Multiple Sclerosis (MS) diagnosis by leveraging Optical Coherence Tomography (OCT) data to assess retinal changes linked to neuro-degeneration. This study analyzed OCT data from 230 MS patients using ML models, including Random Forest (RF), Support Vector Machine (SVM), XGBoost, and k-Nearest Neighbors (KNN), to classify MS severity. RF emerged as the best performer, achieving F1-Scores of 0.74 (left eye) and 0.72 (right eye), with key retinal features such as the Superior and Temporal sectors, Central ILM-RPE, and asymmetry metrics identified as critical predictors. However, challenges remain, including measurement variability across OCT devices and segmentation inconsistencies, showing the need for standardization. Explore the modeling resources here"
  },
  {
    "objectID": "posts/Tidy Tuesday - Fatal Cars/index.html",
    "href": "posts/Tidy Tuesday - Fatal Cars/index.html",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "",
    "text": "April 20th commonly known as “4/20” is an informal, globally recognized holiday celebrated by cannabis enthusiasts. In the United States in particular, the day features cultural events centered around marijuana use, public gatherings, education, and cannabis product trade fairs. The most active period typically falls between 4:20 PM and midnight, marking peak celebration hours.\nAs cannabis consumption becomes more mainstream especially in parts of the U.S., the Netherlands, and other regions where its use is legal or decriminalized public safety concerns have emerged. One of the most pressing questions is whether this unofficial holiday correlates with an increase in civil disobedience, particularly impaired driving and fatal car crashes in urban areas.\nPrevious studies have produced mixed results. For example, Harper and Palayew (2019) found no significant association between 4/20 and increased fatal crashes, while Staples and Redelmeier (2018) reported a noticeable spike in fatal traffic incidents on this date 12.\nAs part of the Tidy Tuesday data series, I explored this issue using several decades of U.S. traffic fatality data to gain deeper insight into the potential risks associated with 4/20 celebrations.\n\n\n\nDo fatal car crashes increase during the 4/20 celebration window (April 20th, 4:20 PM to 11:59 PM)?\nAre weekends more dangerous than weekdays when it comes to fatal traffic crashes?\nAre there noticeable patterns by season or day of the week in fatal crash data?\n\nUsing basic exploratory data analysis, visualization, and modeling, we shall run basic analysis on this data set. We start off by loading the packages and the data. The two tables (daily_accidents and daily_accidents_420) provide daily fatal crash counts, with the latter specifically identifying 4/20 dates.\n\nlibrary(tidytuesdayR)\nlibrary(GWalkR)\nlibrary(tidyverse)\n\nWe go ahead and extract the day of the week and month for each date to be able to analyze the temporal patterns in fatalities, latter on in the analysis.\n\n# load data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# create days\ndaily_accidents$days &lt;- weekdays(daily_accidents$date)\ndaily_accidents$months &lt;- factor(months(daily_accidents$date))\ndaily_accidents_420$days &lt;- weekdays(daily_accidents_420$date)\ndaily_accidents_420$months &lt;- factor(months(daily_accidents_420$date))\n\nNow that we have the day variable we would like to check for the general trend of fatalities on the different days of the week? to do this we calculate the mean number of daily fatalities for each weekday to identify patterns.\n\n# total accidents by day\nweekday_summary &lt;- daily_accidents_420 %&gt;%\n  group_by(days) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_fatalities))\n\nThe bar plot illustrates the average number of fatal car crashes for each day of the week. From the chart, we observe that weekend days, particularly Saturday and Sunday, along with Friday, tend to have the highest average fatalities. This pattern likely reflects increased recreational travel, social activities, and potentially higher rates of bad driving during these days.\n\n# average fatal car crashes\nggplot(weekday_summary, \n       aes(x = reorder(days, -avg_fatalities), y = avg_fatalities)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Weekday\", \n       y = \"Average Fatalities\", \n       title = \"Average Fatal Car Crashes by Weekday\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn contrast, the averages for Monday, Tuesday, Wednesday, and Thursday are noticeably lower and relatively similar to each other. While this difference is not directly explained by the dataset, one possible interpretation is that drivers may be more focused or cautious during the traditional workweek. However, it’s important to note that this is a speculative explanation and should not be taken as a conclusion drawn from the data itself, but rather as a contextual consideration that might warrant further investigation.\nTo understand how fatal car crashes have changed over time, we began by categorizing each day as either a weekday or weekend, and labeling April 20th (4/20) as a holiday. This allows us to structure the data in a way that enables meaningful comparisons between regular weekdays, weekends, and the 4/20 holiday.\n\n# indicators for weekend/weekday and 4/20\ndaily_accidents &lt;- daily_accidents %&gt;%\n  mutate(year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"))\n\ndaily_accidents_420 &lt;- daily_accidents_420 %&gt;%\n  mutate(days = weekdays(date),\n         year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"),\n         holiday = ifelse(e420 == TRUE, \"4/20\", \"Non-4/20\"))\ndaily_accidents$label &lt;- ifelse(daily_accidents$is_weekend, \"Weekend\", \"Weekday\")\ndaily_accidents_420$label &lt;- ifelse(daily_accidents_420$is_weekend, \"Weekend\", \"Weekday\")\n\nNext, we calculated the average number of fatalities per day, grouped by year and day type (weekday or weekend). We then visualized this data using a line plot to observe trends across the years.\n\n# average fatalities per day by year and label\nyear_fatal &lt;- daily_accidents_420 %&gt;%\n  group_by(year, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = 'drop')\n\n\nggplot(year_fatal, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(linewidth = 0.2) +\n  geom_point(size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  theme_test() +\n  scale_x_continuous(breaks = year_fatal$year, \n                     labels = year_fatal$year) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe resulting plot shows several key patterns:\n\nWeekends consistently show higher average fatality rates than weekdays. This is consistent with expectations, as weekends are often associated with more travel, leisure activities, and potentially bad driving.\nFrom 1992 to around 2005, both weekday and weekend fatalities were relatively high.\nThere was a decline in average daily fatalities from about 2005 to 2014, indicating a period of improved road safety.\nHowever, starting in 2014, the average fatalities began to increase again, and this trend continues in more recent years.\nImportantly, even during periods when overall fatalities declined, weekends still maintained higher averages compared to weekdays.\n\nWith this we can answer the question “Are weekends more dangerous than weekdays when it comes to fatal traffic crashes?” suggesting that weekends are consistently associated with a higher number of fatal crashes, regardless of the year.\nWe now shift our focus to a central question: Is April 20th (4/20) associated with a higher number of fatal car crashes compared to other days of the year? Given the cultural significance of 4/20 as a cannabis-focused celebration, it is reasonable to question whether increased substance use on this day contributes to increased crash rates.\nTo examine this, we grouped the dataset into two categories days that fall on 4/20 and all other calendar dates. Each of these groups was then subdivided based on whether the day was a weekday or a weekend. We calculated the average number of fatalities in each subgroup and visualized the results using a comparative bar chart.\n\n# Compare 4/20 vs non-4/20\nholiday_420 &lt;- daily_accidents_420 %&gt;%\n  group_by(holiday, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE),\n            count = n(),\n            .groups = \"drop\")\n\nThe plot reveals that fatal crash averages on 4/20 are actually lower than those on non-4/20 dates. This difference is especially pronounced on weekends, where one might expect the influence of recreational activity and bad driving to be higher. Contrary to popular assumptions, the data suggests that 4/20 does not experience a spike in fatal traffic incidents. In fact, the day tends to see slightly fewer fatalities on average.\n\n# plot\nggplot(holiday_420, aes(x = holiday, y = avg_fatalities, fill = label)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.6) +\n  geom_text(aes(label = round(avg_fatalities, 2)), \n            position = position_dodge(width = 0.7), \n            vjust = -0.5, size = 3.5) +\n  labs(\n    title = \"Average Fatalities on 4/20 vs Other Days\",\n    x = \"Day Category\",\n    y = \"Avg. Fatalities per Day\",\n    fill = \"Day Type\") +\n  theme_test()\n\n\n\n\n\n\n\n\nThese observations challenge the widely held perception that cannabis-related events on 4/20 lead to more dangerous roads. While the results do not eliminate the possibility of localized risks or isolated incidents, the broader national pattern does not support the idea that 4/20 is a particularly deadly day for drivers. To deepen our understanding of how April 20th compares to other days in terms of fatal crashes over time, we plotted the trend of average daily fatalities across all years, distinguishing between weekdays (in green), weekends (in blue), and April 20th specifically (in red). This allowed us to show how 4/20 fatalities track against broader national trends on regular days.\n\nyear_fatal_420 &lt;- daily_accidents_420 %&gt;%\n  filter(month(date) == 4 & day(date) == 20) %&gt;%\n  group_by(year = year(date)) %&gt;%\n  summarise(average_420_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = \"drop\")\n\nThe resulting plot offers a clear comparison. Across most years, the red dashed line representing the average number of fatalities on April 20th consistently falls below both weekday and weekend trends. This reinforces our earlier observation that 4/20 is not associated with a surge in fatal crashes, despite the day’s cultural reputation. In fact, while fatal crashes tend to spike on weekends and occasionally fluctuate over the years, 4/20’s average remains relatively stable and often lower than typical weekend averages.\n\nyear_fatal_with_420 &lt;- year_fatal %&gt;%\n  left_join(year_fatal_420, by = \"year\")\n\n# plot\nggplot(year_fatal_with_420, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(size = 0.2) +\n  geom_point(size = 1) +\n  geom_line(aes(x = year, y = average_420_fatalities), \n            color = \"red\", linetype = \"dashed\", size = 0.2) + \n  geom_point(aes(x = year, y = average_420_fatalities), \n             color = \"red\", size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  \n  theme_test() +\n  scale_color_manual(\n    values = c(\"Weekend\" = \"blue\", \"Weekday\" = \"green\", \"4/20 Average\" = \"red\"),\n    labels = c(\"Weekend\", \"Weekday\", \"4/20 Average\")) +\n  scale_x_continuous(breaks = year_fatal_with_420$year, \n                     labels = year_fatal_with_420$year) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\") +\n    geom_text(aes(x = max(year), y = max(average_420_fatalities), \n                  label = \"Average 4/20 Fatalities\"),\n              color = \"black\",\n              size = 4,\n              hjust = 1,\n              vjust = 6.5)\n\n\n\n\n\n\n\n\nThis strengthens our conclusion: April 20th does not exhibit a pattern of elevated fatal traffic incidents over the years when compared to the national averages for both weekdays and weekends. It directly answers our initial research question about whether 4/20 sees an increase in fatal crashes, it does not. Instead, the data suggest that public fears around cannabis-related driving fatalities on this day may be overstated, at least when looking at national trends over several decades.\nFollowing the exploratory trend analysis, we move into statistical modeling to test the impact of specific factors particularly weekends and the date April 20th (4/20) on the number of daily fatal traffic crashes. The goal here is to determine whether 4/20 is statistically associated with a significantly higher or lower risk of fatal crashes after accounting for other known influences like weekends, which we’ve already seen tend to have higher fatality rates.\nBefore applying any model, we first examine whether the number of daily fatalities (fatalities_count) meets assumptions required for common parametric tests. We start by checking normality across weekdays using QQ plots. The QQ plots show to some extent we can assume that the deviations from the normal distribution are not so off, which implies that daily fatalities are can be said to be normally distributed across days of the week.\n\nlibrary(lubridate)\nlibrary(car)\nlibrary(ggpubr)\n\n# check normality of each weekday group\nggqqplot(daily_accidents_420, \n         x = \"fatalities_count\", \n         facet.by = \"days\")\n\n\n\n\n\n\n\n\nWe would also want to complement this, we use Levene’s Test to check for homogeneity of variances across the weekday groups. Levene’s Test indicates that the assumption of equal variances is violated (significant p-value), meaning the variability in fatal crashes differs across days. This rules out traditional parametric methods.\n\n# Levene's Test for equal variances\nleveneTest(fatalities_count ~ days, data = daily_accidents_420)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    6  49.138 &lt; 2.2e-16 ***\n      9150                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the normality assumption was passed but equal variance assumptions are violated, we use the Kruskal-Wallis test, a non-parametric alternative to ANOVA that does not assume a specific distribution or equal variances.\n\nkruskal.test(fatalities_count ~ days, data = daily_accidents_420)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fatalities_count by days\nKruskal-Wallis chi-squared = 2742.3, df = 6, p-value &lt; 2.2e-16\n\n\nThe Kruskal-Wallis test result shows a highly significant p-value (&lt; 0.01), meaning that at least one day of the week has a statistically different distribution of fatal crashes. This further confirms the importance of day of week patterns in the cases of fatalities.\nNext, to evaluate the effect of weekends (is_weekend) and April 20th (e420) on daily fatal crash counts in a more controlled and quantifiable way, we turn to Poisson regression. Poisson models are commonly used when modeling count data, such as the number of fatal crashes per day.\n\n# Do Holidays or Weekends Have Higher Risk?\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(poissonreg)\n\npoisson &lt;- poisson_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(fatalities_count ~ is_weekend + e420, \n      data = daily_accidents_420)\npoisson\n\nparsnip model object\n\n\nCall:  stats::glm(formula = fatalities_count ~ is_weekend + e420, family = stats::poisson, \n    data = data)\n\nCoefficients:\n   (Intercept)  is_weekendTRUE        e420TRUE  \n        4.9190          0.1873         -0.9751  \n\nDegrees of Freedom: 9156 Total (i.e. Null);  9154 Residual\nNull Deviance:      71490 \nResidual Deviance: 59670    AIC: 121800\n\n\nThe Poisson model output shows:\nThe intercept represents the log expected count of fatalities on a typical weekday that is not 4/20.\n\nexp(4.92) ≈ 137, meaning we expect around 137 fatalities on an average non-weekend, non-4/20 day.\nThe weekend effect (is_weekendTRUE):\n\nexp(0.187) ≈ 1.206\nFatal crashes are 20.6% higher on weekends than on weekdays, a statistically significant difference.\n\nThe April 20th effect (e420TRUE):\n\nexp(-0.972) ≈ 0.377\nApril 20th is associated with a 62.3% decrease in fatal crashes, even after accounting for whether it’s a weekend. This is statistically significant and contradicts the common assumption that 4/20 is more dangerous.\n\n\nTo test whether our data violates this assumption, we check for over dispersion using the check_overdispersion() function.\n\n# check for overdispersion\nperformance::check_overdispersion(poisson$fit)\n\n# Overdispersion test\n\n       dispersion ratio =     6.529\n  Pearson's Chi-Squared = 59768.882\n                p-value =   &lt; 0.001\n\n\nThe result shows a dispersion ratio of 6.529 and a very small p-value (&lt; 0.001), indicating that the variance in fatality counts is much greater than the mean a classic sign of over dispersion. This invalidates the Poisson model, as it will underestimate standard errors and potentially lead to misleading significance tests. Given the over dispersion, we shift to a Negative Binomial Regression, which is more appropriate because it introduces an extra parameter to model the variance independently from the mean.\n\nlibrary(MASS)\n\nnb_model &lt;- glm.nb(fatalities_count ~ is_weekend + e420, data = daily_accidents_420)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = fatalities_count ~ is_weekend + e420, data = daily_accidents_420, \n    init.theta = 25.98766094, link = log)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     4.919055   0.002649 1857.28   &lt;2e-16 ***\nis_weekendTRUE  0.187202   0.004903   38.18   &lt;2e-16 ***\ne420TRUE       -0.972427   0.047733  -20.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(25.9877) family taken to be 1)\n\n    Null deviance: 11070.0  on 9156  degrees of freedom\nResidual deviance:  9221.3  on 9154  degrees of freedom\nAIC: 88473\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  25.988 \n          Std. Err.:  0.453 \n\n 2 x log-likelihood:  -88464.612 \n\n\nThe results from the Negative Binomial model are consistent with earlier findings but provide improved robustness due to better model assumptions. The model fit statistics show marked improvement. The AIC drops significantly from 121,800 in the Poisson model to 88,473 in the Negative Binomial model, and the residual deviance also decreases, indicating a much better fit to the data. These improvements validate the use of the Negative Binomial model for this analysis.\n\ntidy(nb_model, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     137.      0.00265    1857.  0         136.      138.   \n2 is_weekendTRUE    1.21    0.00490      38.2 0           1.19      1.22 \n3 e420TRUE          0.378   0.0477      -20.4 2.95e-92    0.345     0.415\n\n\n\n\n\nThis statistical modeling confirms and strengthens the earlier descriptive findings:\n\nWeekends are significantly more dangerous in terms of fatal crashes, as expected. They are associated with a 20.6% increase in fatal crashes (IRR ≈ 1.206).\nApril 20th does not increase fatal crash risk. On the contrary, the data consistently show that 4/20 is associated with a lower average number of fatal crashes, even when controlling for weekends and other factors. They are associated with a 62.2% decrease in fatalities (IRR ≈ 0.378).\n\nTherefore, the popular belief that 4/20 is a particularly dangerous day on U.S. roads is not supported by the data. In fact, this date shows a statistically significant reduction in fatalities, suggesting that public perception and media narratives about cannabis-related crashes on this day may be misleading.\nWhy might that be?\nSeveral factors could contribute to the lower fatality rate on 4/20, such as:\n\nIncreased public awareness and media attention\nGreater law enforcement presence\nFewer people driving (possibly choosing to stay home for cannabis-related events)\n\nThis analysis is a good reminder that data beats assumption, and that public discourse around events like 4/20 may not always align with the numbers."
  },
  {
    "objectID": "posts/Tidy Tuesday - Fatal Cars/index.html#introduction",
    "href": "posts/Tidy Tuesday - Fatal Cars/index.html#introduction",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "",
    "text": "April 20th commonly known as “4/20” is an informal, globally recognized holiday celebrated by cannabis enthusiasts. In the United States in particular, the day features cultural events centered around marijuana use, public gatherings, education, and cannabis product trade fairs. The most active period typically falls between 4:20 PM and midnight, marking peak celebration hours.\nAs cannabis consumption becomes more mainstream especially in parts of the U.S., the Netherlands, and other regions where its use is legal or decriminalized public safety concerns have emerged. One of the most pressing questions is whether this unofficial holiday correlates with an increase in civil disobedience, particularly impaired driving and fatal car crashes in urban areas.\nPrevious studies have produced mixed results. For example, Harper and Palayew (2019) found no significant association between 4/20 and increased fatal crashes, while Staples and Redelmeier (2018) reported a noticeable spike in fatal traffic incidents on this date 12.\nAs part of the Tidy Tuesday data series, I explored this issue using several decades of U.S. traffic fatality data to gain deeper insight into the potential risks associated with 4/20 celebrations.\n\n\n\nDo fatal car crashes increase during the 4/20 celebration window (April 20th, 4:20 PM to 11:59 PM)?\nAre weekends more dangerous than weekdays when it comes to fatal traffic crashes?\nAre there noticeable patterns by season or day of the week in fatal crash data?\n\nUsing basic exploratory data analysis, visualization, and modeling, we shall run basic analysis on this data set. We start off by loading the packages and the data. The two tables (daily_accidents and daily_accidents_420) provide daily fatal crash counts, with the latter specifically identifying 4/20 dates.\n\nlibrary(tidytuesdayR)\nlibrary(GWalkR)\nlibrary(tidyverse)\n\nWe go ahead and extract the day of the week and month for each date to be able to analyze the temporal patterns in fatalities, latter on in the analysis.\n\n# load data\ntuesdata &lt;- tidytuesdayR::tt_load('2025-04-22')\ndaily_accidents &lt;- tuesdata$daily_accidents\ndaily_accidents_420 &lt;- tuesdata$daily_accidents_420\n\n# create days\ndaily_accidents$days &lt;- weekdays(daily_accidents$date)\ndaily_accidents$months &lt;- factor(months(daily_accidents$date))\ndaily_accidents_420$days &lt;- weekdays(daily_accidents_420$date)\ndaily_accidents_420$months &lt;- factor(months(daily_accidents_420$date))\n\nNow that we have the day variable we would like to check for the general trend of fatalities on the different days of the week? to do this we calculate the mean number of daily fatalities for each weekday to identify patterns.\n\n# total accidents by day\nweekday_summary &lt;- daily_accidents_420 %&gt;%\n  group_by(days) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_fatalities))\n\nThe bar plot illustrates the average number of fatal car crashes for each day of the week. From the chart, we observe that weekend days, particularly Saturday and Sunday, along with Friday, tend to have the highest average fatalities. This pattern likely reflects increased recreational travel, social activities, and potentially higher rates of bad driving during these days.\n\n# average fatal car crashes\nggplot(weekday_summary, \n       aes(x = reorder(days, -avg_fatalities), y = avg_fatalities)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(x = \"Weekday\", \n       y = \"Average Fatalities\", \n       title = \"Average Fatal Car Crashes by Weekday\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn contrast, the averages for Monday, Tuesday, Wednesday, and Thursday are noticeably lower and relatively similar to each other. While this difference is not directly explained by the dataset, one possible interpretation is that drivers may be more focused or cautious during the traditional workweek. However, it’s important to note that this is a speculative explanation and should not be taken as a conclusion drawn from the data itself, but rather as a contextual consideration that might warrant further investigation.\nTo understand how fatal car crashes have changed over time, we began by categorizing each day as either a weekday or weekend, and labeling April 20th (4/20) as a holiday. This allows us to structure the data in a way that enables meaningful comparisons between regular weekdays, weekends, and the 4/20 holiday.\n\n# indicators for weekend/weekday and 4/20\ndaily_accidents &lt;- daily_accidents %&gt;%\n  mutate(year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"))\n\ndaily_accidents_420 &lt;- daily_accidents_420 %&gt;%\n  mutate(days = weekdays(date),\n         year = year(date),\n         is_weekend = days %in% c(\"Saturday\", \"Sunday\"),\n         holiday = ifelse(e420 == TRUE, \"4/20\", \"Non-4/20\"))\ndaily_accidents$label &lt;- ifelse(daily_accidents$is_weekend, \"Weekend\", \"Weekday\")\ndaily_accidents_420$label &lt;- ifelse(daily_accidents_420$is_weekend, \"Weekend\", \"Weekday\")\n\nNext, we calculated the average number of fatalities per day, grouped by year and day type (weekday or weekend). We then visualized this data using a line plot to observe trends across the years.\n\n# average fatalities per day by year and label\nyear_fatal &lt;- daily_accidents_420 %&gt;%\n  group_by(year, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = 'drop')\n\n\nggplot(year_fatal, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(linewidth = 0.2) +\n  geom_point(size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  theme_test() +\n  scale_x_continuous(breaks = year_fatal$year, \n                     labels = year_fatal$year) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe resulting plot shows several key patterns:\n\nWeekends consistently show higher average fatality rates than weekdays. This is consistent with expectations, as weekends are often associated with more travel, leisure activities, and potentially bad driving.\nFrom 1992 to around 2005, both weekday and weekend fatalities were relatively high.\nThere was a decline in average daily fatalities from about 2005 to 2014, indicating a period of improved road safety.\nHowever, starting in 2014, the average fatalities began to increase again, and this trend continues in more recent years.\nImportantly, even during periods when overall fatalities declined, weekends still maintained higher averages compared to weekdays.\n\nWith this we can answer the question “Are weekends more dangerous than weekdays when it comes to fatal traffic crashes?” suggesting that weekends are consistently associated with a higher number of fatal crashes, regardless of the year.\nWe now shift our focus to a central question: Is April 20th (4/20) associated with a higher number of fatal car crashes compared to other days of the year? Given the cultural significance of 4/20 as a cannabis-focused celebration, it is reasonable to question whether increased substance use on this day contributes to increased crash rates.\nTo examine this, we grouped the dataset into two categories days that fall on 4/20 and all other calendar dates. Each of these groups was then subdivided based on whether the day was a weekday or a weekend. We calculated the average number of fatalities in each subgroup and visualized the results using a comparative bar chart.\n\n# Compare 4/20 vs non-4/20\nholiday_420 &lt;- daily_accidents_420 %&gt;%\n  group_by(holiday, label) %&gt;%\n  summarise(avg_fatalities = mean(fatalities_count, na.rm = TRUE),\n            count = n(),\n            .groups = \"drop\")\n\nThe plot reveals that fatal crash averages on 4/20 are actually lower than those on non-4/20 dates. This difference is especially pronounced on weekends, where one might expect the influence of recreational activity and bad driving to be higher. Contrary to popular assumptions, the data suggests that 4/20 does not experience a spike in fatal traffic incidents. In fact, the day tends to see slightly fewer fatalities on average.\n\n# plot\nggplot(holiday_420, aes(x = holiday, y = avg_fatalities, fill = label)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.6) +\n  geom_text(aes(label = round(avg_fatalities, 2)), \n            position = position_dodge(width = 0.7), \n            vjust = -0.5, size = 3.5) +\n  labs(\n    title = \"Average Fatalities on 4/20 vs Other Days\",\n    x = \"Day Category\",\n    y = \"Avg. Fatalities per Day\",\n    fill = \"Day Type\") +\n  theme_test()\n\n\n\n\n\n\n\n\nThese observations challenge the widely held perception that cannabis-related events on 4/20 lead to more dangerous roads. While the results do not eliminate the possibility of localized risks or isolated incidents, the broader national pattern does not support the idea that 4/20 is a particularly deadly day for drivers. To deepen our understanding of how April 20th compares to other days in terms of fatal crashes over time, we plotted the trend of average daily fatalities across all years, distinguishing between weekdays (in green), weekends (in blue), and April 20th specifically (in red). This allowed us to show how 4/20 fatalities track against broader national trends on regular days.\n\nyear_fatal_420 &lt;- daily_accidents_420 %&gt;%\n  filter(month(date) == 4 & day(date) == 20) %&gt;%\n  group_by(year = year(date)) %&gt;%\n  summarise(average_420_fatalities = mean(fatalities_count, na.rm = TRUE), .groups = \"drop\")\n\nThe resulting plot offers a clear comparison. Across most years, the red dashed line representing the average number of fatalities on April 20th consistently falls below both weekday and weekend trends. This reinforces our earlier observation that 4/20 is not associated with a surge in fatal crashes, despite the day’s cultural reputation. In fact, while fatal crashes tend to spike on weekends and occasionally fluctuate over the years, 4/20’s average remains relatively stable and often lower than typical weekend averages.\n\nyear_fatal_with_420 &lt;- year_fatal %&gt;%\n  left_join(year_fatal_420, by = \"year\")\n\n# plot\nggplot(year_fatal_with_420, aes(x = year, y = avg_fatalities, color = label)) +\n  geom_line(size = 0.2) +\n  geom_point(size = 1) +\n  geom_line(aes(x = year, y = average_420_fatalities), \n            color = \"red\", linetype = \"dashed\", size = 0.2) + \n  geom_point(aes(x = year, y = average_420_fatalities), \n             color = \"red\", size = 1) +\n  labs(title = \"Average Daily U.S. Fatalities by Year: Weekday vs Weekend\",\n       x = \"Year\",\n       y = \"Avg. Fatalities per Day\",\n       color = \"Day Type\") +\n  \n  theme_test() +\n  scale_color_manual(\n    values = c(\"Weekend\" = \"blue\", \"Weekday\" = \"green\", \"4/20 Average\" = \"red\"),\n    labels = c(\"Weekend\", \"Weekday\", \"4/20 Average\")) +\n  scale_x_continuous(breaks = year_fatal_with_420$year, \n                     labels = year_fatal_with_420$year) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\") +\n    geom_text(aes(x = max(year), y = max(average_420_fatalities), \n                  label = \"Average 4/20 Fatalities\"),\n              color = \"black\",\n              size = 4,\n              hjust = 1,\n              vjust = 6.5)\n\n\n\n\n\n\n\n\nThis strengthens our conclusion: April 20th does not exhibit a pattern of elevated fatal traffic incidents over the years when compared to the national averages for both weekdays and weekends. It directly answers our initial research question about whether 4/20 sees an increase in fatal crashes, it does not. Instead, the data suggest that public fears around cannabis-related driving fatalities on this day may be overstated, at least when looking at national trends over several decades.\nFollowing the exploratory trend analysis, we move into statistical modeling to test the impact of specific factors particularly weekends and the date April 20th (4/20) on the number of daily fatal traffic crashes. The goal here is to determine whether 4/20 is statistically associated with a significantly higher or lower risk of fatal crashes after accounting for other known influences like weekends, which we’ve already seen tend to have higher fatality rates.\nBefore applying any model, we first examine whether the number of daily fatalities (fatalities_count) meets assumptions required for common parametric tests. We start by checking normality across weekdays using QQ plots. The QQ plots show to some extent we can assume that the deviations from the normal distribution are not so off, which implies that daily fatalities are can be said to be normally distributed across days of the week.\n\nlibrary(lubridate)\nlibrary(car)\nlibrary(ggpubr)\n\n# check normality of each weekday group\nggqqplot(daily_accidents_420, \n         x = \"fatalities_count\", \n         facet.by = \"days\")\n\n\n\n\n\n\n\n\nWe would also want to complement this, we use Levene’s Test to check for homogeneity of variances across the weekday groups. Levene’s Test indicates that the assumption of equal variances is violated (significant p-value), meaning the variability in fatal crashes differs across days. This rules out traditional parametric methods.\n\n# Levene's Test for equal variances\nleveneTest(fatalities_count ~ days, data = daily_accidents_420)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    6  49.138 &lt; 2.2e-16 ***\n      9150                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause the normality assumption was passed but equal variance assumptions are violated, we use the Kruskal-Wallis test, a non-parametric alternative to ANOVA that does not assume a specific distribution or equal variances.\n\nkruskal.test(fatalities_count ~ days, data = daily_accidents_420)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fatalities_count by days\nKruskal-Wallis chi-squared = 2742.3, df = 6, p-value &lt; 2.2e-16\n\n\nThe Kruskal-Wallis test result shows a highly significant p-value (&lt; 0.01), meaning that at least one day of the week has a statistically different distribution of fatal crashes. This further confirms the importance of day of week patterns in the cases of fatalities.\nNext, to evaluate the effect of weekends (is_weekend) and April 20th (e420) on daily fatal crash counts in a more controlled and quantifiable way, we turn to Poisson regression. Poisson models are commonly used when modeling count data, such as the number of fatal crashes per day.\n\n# Do Holidays or Weekends Have Higher Risk?\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(poissonreg)\n\npoisson &lt;- poisson_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(fatalities_count ~ is_weekend + e420, \n      data = daily_accidents_420)\npoisson\n\nparsnip model object\n\n\nCall:  stats::glm(formula = fatalities_count ~ is_weekend + e420, family = stats::poisson, \n    data = data)\n\nCoefficients:\n   (Intercept)  is_weekendTRUE        e420TRUE  \n        4.9190          0.1873         -0.9751  \n\nDegrees of Freedom: 9156 Total (i.e. Null);  9154 Residual\nNull Deviance:      71490 \nResidual Deviance: 59670    AIC: 121800\n\n\nThe Poisson model output shows:\nThe intercept represents the log expected count of fatalities on a typical weekday that is not 4/20.\n\nexp(4.92) ≈ 137, meaning we expect around 137 fatalities on an average non-weekend, non-4/20 day.\nThe weekend effect (is_weekendTRUE):\n\nexp(0.187) ≈ 1.206\nFatal crashes are 20.6% higher on weekends than on weekdays, a statistically significant difference.\n\nThe April 20th effect (e420TRUE):\n\nexp(-0.972) ≈ 0.377\nApril 20th is associated with a 62.3% decrease in fatal crashes, even after accounting for whether it’s a weekend. This is statistically significant and contradicts the common assumption that 4/20 is more dangerous.\n\n\nTo test whether our data violates this assumption, we check for over dispersion using the check_overdispersion() function.\n\n# check for overdispersion\nperformance::check_overdispersion(poisson$fit)\n\n# Overdispersion test\n\n       dispersion ratio =     6.529\n  Pearson's Chi-Squared = 59768.882\n                p-value =   &lt; 0.001\n\n\nThe result shows a dispersion ratio of 6.529 and a very small p-value (&lt; 0.001), indicating that the variance in fatality counts is much greater than the mean a classic sign of over dispersion. This invalidates the Poisson model, as it will underestimate standard errors and potentially lead to misleading significance tests. Given the over dispersion, we shift to a Negative Binomial Regression, which is more appropriate because it introduces an extra parameter to model the variance independently from the mean.\n\nlibrary(MASS)\n\nnb_model &lt;- glm.nb(fatalities_count ~ is_weekend + e420, data = daily_accidents_420)\nsummary(nb_model)\n\n\nCall:\nglm.nb(formula = fatalities_count ~ is_weekend + e420, data = daily_accidents_420, \n    init.theta = 25.98766094, link = log)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     4.919055   0.002649 1857.28   &lt;2e-16 ***\nis_weekendTRUE  0.187202   0.004903   38.18   &lt;2e-16 ***\ne420TRUE       -0.972427   0.047733  -20.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(25.9877) family taken to be 1)\n\n    Null deviance: 11070.0  on 9156  degrees of freedom\nResidual deviance:  9221.3  on 9154  degrees of freedom\nAIC: 88473\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  25.988 \n          Std. Err.:  0.453 \n\n 2 x log-likelihood:  -88464.612 \n\n\nThe results from the Negative Binomial model are consistent with earlier findings but provide improved robustness due to better model assumptions. The model fit statistics show marked improvement. The AIC drops significantly from 121,800 in the Poisson model to 88,473 in the Negative Binomial model, and the residual deviance also decreases, indicating a much better fit to the data. These improvements validate the use of the Negative Binomial model for this analysis.\n\ntidy(nb_model, exponentiate = TRUE, conf.int = TRUE)\n\n# A tibble: 3 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     137.      0.00265    1857.  0         136.      138.   \n2 is_weekendTRUE    1.21    0.00490      38.2 0           1.19      1.22 \n3 e420TRUE          0.378   0.0477      -20.4 2.95e-92    0.345     0.415\n\n\n\n\n\nThis statistical modeling confirms and strengthens the earlier descriptive findings:\n\nWeekends are significantly more dangerous in terms of fatal crashes, as expected. They are associated with a 20.6% increase in fatal crashes (IRR ≈ 1.206).\nApril 20th does not increase fatal crash risk. On the contrary, the data consistently show that 4/20 is associated with a lower average number of fatal crashes, even when controlling for weekends and other factors. They are associated with a 62.2% decrease in fatalities (IRR ≈ 0.378).\n\nTherefore, the popular belief that 4/20 is a particularly dangerous day on U.S. roads is not supported by the data. In fact, this date shows a statistically significant reduction in fatalities, suggesting that public perception and media narratives about cannabis-related crashes on this day may be misleading.\nWhy might that be?\nSeveral factors could contribute to the lower fatality rate on 4/20, such as:\n\nIncreased public awareness and media attention\nGreater law enforcement presence\nFewer people driving (possibly choosing to stay home for cannabis-related events)\n\nThis analysis is a good reminder that data beats assumption, and that public discourse around events like 4/20 may not always align with the numbers."
  },
  {
    "objectID": "posts/Tidy Tuesday - Fatal Cars/index.html#footnotes",
    "href": "posts/Tidy Tuesday - Fatal Cars/index.html#footnotes",
    "title": "Is 4/20 Dangerous for Driving? A Look at U.S. Fatal Car Crashes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHarper S, Palayew A. The annual cannabis holiday and fatal traffic crashes. BMJ Injury Prevention. Published Online First: 29 January 2019. https://doi.org/10.1136/injuryprev-2018-043068. Manuscript and data/code↩︎\nStaples JA, Redelmeier DA. The April 20 cannabis celebration and fatal traffic crashes in the United States. JAMA Intern Med. 2018;178(4):569–572. https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2672751↩︎"
  },
  {
    "objectID": "posts/Mixture Models/index.html",
    "href": "posts/Mixture Models/index.html",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Mixture Models/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "href": "posts/Mixture Models/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  }
]