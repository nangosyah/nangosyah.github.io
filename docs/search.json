[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nAfter 6 years of employment, I decided to take a two-year break to pursue my Master’s in Statistics and Data Science. The past few years have been an immersive learning experience, allowing me grow my analytical and statistical skills while diving deep into the health sciences field. But let me take you back to where it all started.\n\nThe Start: Research, Data Analytics, and Business Advisory\nMy professional journey started with me working as a Research Associate at Inachee. Where I worked with a team that conducted business sector analysis, financial appraisals, business plans and feasibility studies. It was here that I developed a passion for translating data into actionable business insights. This role introduced me to data management, as I was responsible for overseeing the curating and development of a cloud-based data bank where the company would market and sale its research assets to customers, and I began to understand the power of data-driven decisions.\nFrom Inachee, I transitioned into EnerGrow, a startup that focuses on asset financing for SMEs in Uganda. Over the years, I held various roles within the company, from Research and Data Analyst to Expansion Manager and ultimately a Research and Development Consultant. I was fortunate to lead the design of a pilot asset financing loan program and manage stakeholder reporting and partnerships on projects financed by organizations like EEP, Doen Foundation, GIZ, MECs Co., IMEU, EnDev, CEI and Rockefeller Foundation. These roles shaped my expertise in managing and analyzing data, building reports, and managing projects. I was able to work across multiple functions, helping the company grow and expand to new regions in Uganda.\n\n\nA New Chapter: My Master’s in Data Science\nDespite the exciting projects I was involved in, I felt the urge to dive deeper into the technical side of data analysis. That’s why I took a two-year break to pursue my Master’s degree in Statistics and Data Science at the University of Hasselt in Belgium. My interest in health sciences was sparked by the growing role of data sciences and statistics in solving some of the most complex biological problems through data. I wanted to be part of that innovation and bring my experience in research and analytics into this cutting-edge field.\n\n\nLooking Forward: Open to New Opportunities\nNow that I’m nearing the end of my Master’s, I am more than ready to apply my expanded skillset in data science, statistics, and bioinformatics. I’m looking for opportunities that will allow me to continue learning, growing, and making an impact, especially in health sciences or any other sector where data can drive meaningful change.\nI’ve worked in financial services, research, and business advisory, and I’m now keen to pivot into life sciences and related fields. I believe the blend of my past experience with my new technical skills can offer unique perspectives and solutions.\nThis blog will serve as a platform for me to share the projects I’ve been working on, insights from the world of data science, and my journey of navigating this ever-evolving field. Whether you’re here to learn more about data science applications or you’re simply curious about life sciences, I hope you find something interesting.\nAnd if you’re looking for someone with a diverse background in data-driven problem-solving, feel free to reach out, I’m open to exploring new opportunities!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I’m passionate about data-driven impact, with experience in research, project management, and client relations across agriculture, renewable energy, and micro-finance.\nI enjoy translating ideas into action, delivering meaningful end-to-end programs, solving complex challenges with cross-functional teams, fostering collaboration through engaging meetings, and building shared value relationships with communities, partners, and donors/investors.\nOutside of work, I enjoy Music, Football, Formula One and a lot of other sports.\n\nAt the moment, I am exploring roles that focus on:\nHealth Sciences: research, data analytics, bioinformatics\nProject delivery within development sectors\nInnovation: pilots, initiatives, processes\nStrategy & transformation\nData Management\nI am also open to interesting conversations about where I can add value!"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Using the Tidy-models suite for Machine Learning\n\n\n\n\n\n\nR\n\n\nData Science\n\n\nClassification\n\n\nMachine learning\n\n\nData Processing\n\n\nTidy-models\n\n\nTitanic dataset\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nModel Based Clustering of High Dimensional Longitudinal Data\n\n\n\n\n\n\nR\n\n\nModel Based Clustering\n\n\nLinear Mixed Models\n\n\nMixture Models\n\n\nPeptides\n\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\nAug 31, 2024\n\n\nTom Wellard Nangosyah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html",
    "href": "posts/Using Tidy Models for Machine Learning/index.html",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThe content of this tutorial is primarily based on the book “Tidy Modeling with R” by Max Kuhn and Julia Silge (2021). The analysis scheme also follows the approach outlined in the R Classification with Tidymodels tutorial.\n\n\nWe will use the Titanic dataset from Kaggle for our analysis, with the goal of building a model to predict which passengers survived the Titanic shipwreck. We will implement a classification workflow using the tidymodels package, demonstrating how workflows and recipes can be utilized for effective model building. Our research question is:\n“What sorts of people were more likely to survive?”\nTo address this question, we will consider factors such as the number of lifeboats, age, gender, and socio-economic class, based on the Titanic’s sinking on April 15, 1912. We will use classification methods to categorize passengers into those who survived and those who did not. Common classification techniques like logistic regression, random forests and K-nearest neighbors will be employed to optimize the solution with minimal error.\nFirst, we will load the necessary packages for the analysis:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bundle)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(readr)\nlibrary(stacks)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(ggmap)\nlibrary(visdat)\nlibrary(corrr)\nlibrary(ggsignif)\nlibrary(gt)\nlibrary(vip)\nlibrary(themis)\nlibrary(purrr)\nlibrary(keras)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(kknn)\nlibrary(reticulate)\n\nWe import the data for the analysis\n\nttest &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/test.csv\")\nttrain &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/train.csv\")\ntsub &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/gender_submission.csv\")\n\n\n\nTo gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)\n\n\n\n\nAfter applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy.\n\n\n\nTo enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))\n\n\n\n\nWe will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃\n\n\n\n\n\n\n\n\nIn machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\n\n\n\nWe will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model.\n\n\n\n\nTo prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\n\nWe will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived) \n\n\n\n\nIn the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#using-the-tidy-models-suite-for-machine-learning",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#using-the-tidy-models-suite-for-machine-learning",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "Note\n\n\n\nThe content of this tutorial is primarily based on the book “Tidy Modeling with R” by Max Kuhn and Julia Silge (2021). The analysis scheme also follows the approach outlined in the R Classification with Tidymodels tutorial.\n\n\nWe will use the Titanic dataset from Kaggle for our analysis, with the goal of building a model to predict which passengers survived the Titanic shipwreck. We will implement a classification workflow using the tidymodels package, demonstrating how workflows and recipes can be utilized for effective model building. Our research question is:\n“What sorts of people were more likely to survive?”\nTo address this question, we will consider factors such as the number of lifeboats, age, gender, and socio-economic class, based on the Titanic’s sinking on April 15, 1912. We will use classification methods to categorize passengers into those who survived and those who did not. Common classification techniques like logistic regression, random forests and K-nearest neighbors will be employed to optimize the solution with minimal error.\nFirst, we will load the necessary packages for the analysis:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(bundle)\nlibrary(vetiver)\nlibrary(pins)\nlibrary(readr)\nlibrary(stacks)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(ggmap)\nlibrary(visdat)\nlibrary(corrr)\nlibrary(ggsignif)\nlibrary(gt)\nlibrary(vip)\nlibrary(themis)\nlibrary(purrr)\nlibrary(keras)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(kknn)\nlibrary(reticulate)\n\nWe import the data for the analysis\n\nttest &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/test.csv\")\nttrain &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/train.csv\")\ntsub &lt;- read_csv(\"/Users/nangosyah/Documents/Kaggle Data-sets/titanic/gender_submission.csv\")"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#format-data",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#format-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To gain a preliminary understanding of the dataset, we will perform some exploratory data analysis (EDA). We start by examining a few rows from the dataset to get an initial impression of its structure and contents.\n\nglimpse(ttrain)\n\nRows: 891\nColumns: 12\n$ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ Survived    &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1…\n$ Pclass      &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3…\n$ Name        &lt;chr&gt; \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl…\n$ Sex         &lt;chr&gt; \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal…\n$ Age         &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, …\n$ SibSp       &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0…\n$ Parch       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0…\n$ Ticket      &lt;chr&gt; \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37…\n$ Fare        &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,…\n$ Cabin       &lt;chr&gt; NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C…\n$ Embarked    &lt;chr&gt; \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"…\n\n\nAt this stage, we will ensure that the data types are correct, particularly for the target variable Survived, which should be a factor. Similarly, all categorical variables will be converted to factors. We shall align these data conversions both in out testing set and and training set.\n\n# training set\nttrain$Sex &lt;- as.factor(ttrain$Sex)\nttrain$Survived &lt;- as.factor(ttrain$Survived)\nttrain$Pclass &lt;- as.factor(ttrain$Pclass)\nttrain$Embarked &lt;- as.factor(ttrain$Embarked)\n\n# testing set\nttest$Sex &lt;- as.factor(ttest$Sex)\nttest$Pclass &lt;- as.factor(ttest$Pclass)\nttest$Embarked &lt;- as.factor(ttest$Embarked)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#missing-data",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#missing-data",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "After applying the transformations, we will now examine the first 5 records to get an initial sense of the data we’re working with. This allows us to verify the changes and better understand the dataset structure.\n\nttrain %&gt;%\n  slice_head(n = 5) %&gt;%\n  gt() \n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22\n1\n0\nA/5 21171\n7.2500\nNA\nS\n\n\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26\n0\n0\nSTON/O2. 3101282\n7.9250\nNA\nS\n\n\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35\n1\n0\n113803\n53.1000\nC123\nS\n\n\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35\n0\n0\n373450\n8.0500\nNA\nS\n\n\n\n\n\n\n\nFrom the data, we observe the presence of NA values in the Cabin and Age columns. We will propose methods to handle these missing values in later stages. For now, we will proceed to visualize the data structure to gain insights into its distribution and patterns.\n\nvis_dat(ttrain)\n\n\n\n\n\n\n\n\nThe data format appears to be in good shape after the adjustments made, aside from the missing data (NAs) that still need attention. To assess the extent of missingness, we will now examine the percentage of missing values across the dataset. For this task, we will use functions from the visdat package to visualize and quantify missingness.\n\nvis_miss(ttrain, sort_miss = TRUE)\n\n\n\n\n\n\n\n\nAn alternative method to the same thing could be with the is.na function from base R which can be achieved as below:\n\nis.na(ttrain) %&gt;% colSums()\n\nPassengerId    Survived      Pclass        Name         Sex         Age \n          0           0           0           0           0         177 \n      SibSp       Parch      Ticket        Fare       Cabin    Embarked \n          0           0           0           0         687           2 \n\n\nThe dataset has significant missingness, with 77% missing values for the Cabin variable and 20% missing for Age. This level of missing data can cause issues, particularly for models that don’t handle missingness directly. These missing values will be addressed in later stages to ensure model robustness and accuracy."
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#create-variables",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#create-variables",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To enhance model learning capabilities, we created a new feature: the mean age per class (age_perclass). This feature represents the average age of passengers within each Pclass, providing insight into the typical age distribution by class. Additionally, we used these class-specific means to impute missing values in the Age variable, ensuring that missing ages were replaced with the average age of passengers in the same class.\n\nttrain &lt;- ttrain %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(age_perclass = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), age_perclass, Age))"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-overview",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-overview",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now review the data overview following the manipulations using the skimr package. This package provides a detailed summary of the dataset, including data types, missing values, and summary statistics. Here’s how we’ll proceed:\n\nskim(ttrain)\n\n\nData summary\n\n\nName\nttrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurvived\n0\n1\nFALSE\n2\n0: 549, 1: 342\n\n\nPclass\n0\n1\nFALSE\n3\n3: 491, 1: 216, 2: 184\n\n\nSex\n0\n1\nFALSE\n2\nmal: 577, fem: 314\n\n\nEmbarked\n2\n1\nFALSE\n3\nS: 644, C: 168, Q: 77\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1\n446.00\n257.35\n1.00\n223.50\n446.00\n668.50\n891.00\n▇▇▇▇▇\n\n\nAge\n0\n1\n29.29\n13.21\n0.42\n22.00\n26.00\n37.00\n80.00\n▂▇▃▁▁\n\n\nSibSp\n0\n1\n0.52\n1.10\n0.00\n0.00\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nParch\n0\n1\n0.38\n0.81\n0.00\n0.00\n0.00\n0.00\n6.00\n▇▁▁▁▁\n\n\nFare\n0\n1\n32.20\n49.69\n0.00\n7.91\n14.45\n31.00\n512.33\n▇▁▁▁▁\n\n\nage_perclass\n0\n1\n29.29\n5.38\n25.14\n25.14\n25.14\n29.88\n38.23\n▇▃▁▁▃"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-splitting",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-splitting",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In machine learning, we typically divide the data into a training set and a testing set. The training set is used to fit the models, while the testing set is used to evaluate their performance. To ensure that the training set is representative of the overall dataset, we must correctly partition the initial dataset.\nWe will use a histogram to visualize the distribution of the dependent variable, Survived, in our data split.\n\nttrain %&gt;%\n  ggplot(aes(Survived)) +\n  geom_bar() \n\n\n\n\n\n\n\n\nTo perform the split, we will use the rsample package from the tidymodels suite. This package helps create an object containing information about the split. We will then use the training() and testing() functions to generate the training and test sets.\nHere’s how to do it:\n\nset.seed(123)\n\n# split 3/4 of the data into the training set \ndata_split &lt;- initial_split(ttrain, \n                           prop = 3/4, \n                           strata = Survived)\n\n# two sets\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-exploration",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-exploration",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will explore the training data to gain insights and identify which variables are important for modeling. This process is iterative: we may build a prototype model, analyze the results, and refine the model based on new insights from exploration.\nThis exploration and modeling will be conducted exclusively with the training set. We shall create a copy of the training set so that we don’t alter the data during our exploration phase.\n\nexplore &lt;- data_train\n\nWe will now use the training dataset to explore relationships between predictor variables and the outcome variable, Survived. This exploration will help us identify which variables are most relevant for predicting passenger survival.\n\n\nWe will examine the numerical variables to check fro differences between passengers who survived and those who did not. This will help us understand how these variables vary with survival status.\n\nexplore %&gt;%\n  ggplot(aes(x = Survived, y = Age, \n             fill = Survived, color = Survived)) +\n  geom_boxplot(alpha=0.4) \n\n\n\n\n\n\n\n\nFrom the exploratory data analysis (EDA), we observe that:\n\nSome numerical variables are on different scales.\nSeveral variables exhibit heavy tails and some show bi-modal distributions.\n\nTo prepare the data for modeling, we need to transform these variables to approximate a normal distribution. This will help improve model performance.\nWe will use the variables Age, SibSp, Parch, and Fare as predictors in our model.\n\n\n\nWe go ahead analyse the categorical variables in relation with the dependent variable Survived. We output tables giving us an idea of the grouping in the data.\n\n\n\n\n\n\n\n\nTitanic Survivors\n\n\n0 - Died 1 - Survived\n\n\nSex\nDistricts\nPercent\n\n\n\n\n0\n\n\nfemale\n59.00\n14.36\n\n\nmale\n352.00\n85.64\n\n\n1\n\n\nfemale\n176.00\n68.75\n\n\nmale\n80.00\n31.25\n\n\n\n\n\n\n\n\nexplore %&gt;%\n  ggplot(aes(Survived, Sex)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nFrom the plot, we observe that the majority of passengers who died are male, highlighted in yellow, compared to females. Additionally, a higher proportion of survivors are female. We will also examine if the socio-economic status, indicated by the cabin class, can help distinguish between those who survived and those who did not.\n\nexplore %&gt;%\n  ggplot(aes(Survived, Pclass)) +\n  geom_bin2d() +\n  scale_fill_continuous(type = \"viridis\") \n\n\n\n\n\n\n\n\nThe plot shows that the majority of passengers who died were from the lowest socio-economic class, with Class 3 having the highest number of deaths compared to Classes 1 and 2.\nTherefore, we will include all categorical variables Pclass, Sex, and Embarked—as predictors in our model."
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#data-preparation",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#data-preparation",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "To prepare our data for modeling, we will:\n\nHandle missing values.\nAddress and remove outliers.\nPerform feature selection.\nEngineer new features.\nScale variables.\nCreate a validation set.\n\nWe will use the tidymodels suite, specifically the recipes and workflows packages, for these steps.\n\nrecipes are used for data processing, including:\n\nData cleaning: Fix or remove outliers, fill in missing values, or drop rows/columns with excessive missing data.\nFeature selection: Remove attributes that do not provide useful information.\nFeature scaling: Standardize or normalize features.\nFeature engineering: Discretize continuous features, decompose features (e.g., extract weekday from a date), apply transformations and aggregate features into new, meaningful features.\n\n\nThe recipes package allows us to create reusable objects for data preprocessing that can be applied consistently throughout the modeling process. In the tidymodels framework, this is typically integrated with the workflows package, which combines the preprocessed data (from the recipe) with the chosen model, streamlining the modeling process and ensuring that the same preprocessing steps are applied during both training and evaluation.\nNow to prepare our data from modeling we shall select the variables we shall use in our model.\n\nmodelttrain &lt;-\n  data_train %&gt;%\n  select(\n    PassengerId, Survived, Age, Sex, \n    Pclass, SibSp, Parch,Fare, Embarked)\n\nglimpse(modelttrain)\n\nRows: 667\nColumns: 9\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Age         &lt;dbl&gt; 25.14062, 54.00000, 2.00000, 20.00000, 39.00000, 14.00000,…\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ SibSp       &lt;dbl&gt; 0, 0, 3, 0, 1, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 1, 0, 0, 1, 2…\n$ Parch       &lt;dbl&gt; 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Fare        &lt;dbl&gt; 8.4583, 51.8625, 21.0750, 8.0500, 31.2750, 7.8542, 18.0000…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n\n\nNow that we have our final selected variables for modeling we shall do the initial data split again since we updated the original data.\n\nset.seed(123)\n\ndata_split &lt;- initial_split(modelttrain,\n                           prop = 3/4, \n                           strata = Survived)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\nWith our new data split, we can now create a recipe for data preprocessing. For detailed guidance on various preprocessing techniques, refer to https://www.tmwr.org/pre-proc-table.html. Below is the code to create our recipe:\n\nmodelttrain_recipe &lt;-\n  recipe(Survived ~ .,data = modelttrain) %&gt;%\n  update_role(PassengerId, new_role = \"ID\") %&gt;%\n  step_log(Parch,SibSp,Fare) %&gt;%\n  step_naomit(everything(), skip = TRUE) %&gt;%\n  step_novel(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_numeric(), -all_outcomes(), \n                 -PassengerId) %&gt;%\n  step_zv(all_numeric(), -all_outcomes()) %&gt;%\n  step_corr(all_numeric(), threshold = 0.7, method = \"spearman\") \n\nThe modelttrain_recipe is designed to preprocess the data for modeling a detailed breakdown of each step is given below:\n\nFirst, we define the recipe with recipe(Survived ~ ., data = modelttrain), specifying Survived as the outcome variable and all other columns as predictors using the modelttrain dataset.\nNext, we use update_role(PassengerId, new_role = \"ID\") to use PassengerId as an identifier rather than a predictor. This allows us to keep track of individual records without including PassengerId in the model.\nWe then apply step_log(Parch, SibSp, Fare, Age) to log-transform the skewed numerical variables. This step addresses the skewness in the distributions but note that it can cause issues with negative values.\nTo handle missing values, we use step_naomit(everything(), skip = TRUE), which removes rows with NA or NaN values. The skip = TRUE argument ensures that this step is not applied to new data during model assessment, thus preserving the number of samples.\nThe step_novel(all_nominal(), -all_outcomes()) step converts nominal variables to factors and handles any new levels not seen during training. This ensures that all categorical variables are appropriately processed.\nWe standardize numeric variables using step_normalize(all_numeric(), -all_outcomes(), -PassengerId), which scales predictors to have a mean of zero and a standard deviation of one.\nWe also remove variables with zero variance using step_zv(all_numeric(), -all_outcomes()), as these variables do not provide useful information for modeling.\nFinally, step_corr(all_predictors(), threshold = 0.7, method = \"spearman\") removes predictors that have high correlations (greater than 0.7) with other predictors, this cab reduce problems related to multicollinearity.\n\nOur new data after preprocessing now looks as below:\n\nsummary(modelttrain_recipe)\n\n# A tibble: 9 × 4\n  variable    type      role      source  \n  &lt;chr&gt;       &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 PassengerId &lt;chr [2]&gt; ID        original\n2 Age         &lt;chr [2]&gt; predictor original\n3 Sex         &lt;chr [3]&gt; predictor original\n4 Pclass      &lt;chr [3]&gt; predictor original\n5 SibSp       &lt;chr [2]&gt; predictor original\n6 Parch       &lt;chr [2]&gt; predictor original\n7 Fare        &lt;chr [2]&gt; predictor original\n8 Embarked    &lt;chr [3]&gt; predictor original\n9 Survived    &lt;chr [3]&gt; outcome   original\n\n\nTo verify that our recipe has been applied correctly, we can use the prep() and juice() functions. The prep() function prepares the recipe based on the training data, and the juice() function extracts the processed data to inspect the results.\n\nmodel_data &lt;- \n  modelttrain_recipe %&gt;% \n  prep() %&gt;% \n  juice() \n\nglimpse(model_data)\n\nRows: 665\nColumns: 6\n$ PassengerId &lt;dbl&gt; 6, 7, 8, 13, 14, 15, 19, 21, 25, 27, 28, 31, 36, 38, 41, 4…\n$ Age         &lt;dbl&gt; -0.3073507, 1.8760623, -2.0580997, -0.6962744, 0.7412079, …\n$ Sex         &lt;fct&gt; male, male, male, male, male, female, female, male, female…\n$ Pclass      &lt;fct&gt; 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3…\n$ Embarked    &lt;fct&gt; Q, S, S, S, S, S, S, S, S, C, S, C, S, S, S, S, C, S, Q, C…\n$ Survived    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#validation-set",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#validation-set",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "We will now create a validation set that will be used for hyper-parameter tuning during model training. To achieve this, we apply k-fold cross-validation, which helps in splitting the data into multiple folds for more robust evaluation. We will use the vfold_cv() function to generate a set of validation folds.\n\nset.seed(145)\n\ncv_folds &lt;-\n vfold_cv(modelttrain, \n          v = 5, \n          strata = Survived)"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#model-building",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#model-building",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "",
    "text": "In the model-building process using the tidy-models framework, we follow a structured approach. We begin by selecting the model type, then specify the engine to be used, and finally define the mode, either regression or classification based on the task at hand. We shall specify different models to be used.\n\n\n\nlog_spec &lt;- \n  logistic_reg() %&gt;%\n  set_engine(engine = \"glm\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nrf_spec &lt;- \n  rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n\n\nknn_spec &lt;- \n  nearest_neighbor(neighbors = 4) %&gt;% \n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/Using Tidy Models for Machine Learning/index.html#compare-models",
    "href": "posts/Using Tidy Models for Machine Learning/index.html#compare-models",
    "title": "Using the Tidy-models suite for Machine Learning",
    "section": "Compare Models",
    "text": "Compare Models\nWe now extract the performance metrics from all the fitted models for comparison.\n\n# plot metrics\nggplot(mean_metrics, aes(x = model, y = estimate_value, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        axis.title.y = element_blank()) +\n  geom_text(aes(label = sprintf(\"%.2f\", estimate_value)), \n            position = position_dodge(width = 0.9), \n            vjust = -0.5, \n            size = 3) \n\n\n\n\n\n\n\n\nThe performance across the models is quite similar, with Random Forest performing slightly better. We will now evaluate the final model on the test set.\nTo accomplish this, the last_fit() function from the tidymodels package can be used. This function fits the model to the entire training dataset and evaluates it on the test set. You’ll need to provide the last_fit() function with the workflow object of the best model and the data split object (excluding the training data). This will allow us to obtain the performance metrics for the final model.\n\nlast_fit_rf &lt;- last_fit(rf_wflow, \n                        split = data_split,\n                        metrics = metric_set(\n                          recall, precision, f_meas, \n                          accuracy, kap,\n                          roc_auc, sens, spec)\n                        )\n\nTo display the performance metrics, we will use the collect_metrics() function as previously done.\n\nlast_fit_rf %&gt;%\n  collect_metrics()\n\n# A tibble: 8 × 4\n  .metric   .estimator .estimate .config             \n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 recall    binary         0.903 Preprocessor1_Model1\n2 precision binary         0.769 Preprocessor1_Model1\n3 f_meas    binary         0.830 Preprocessor1_Model1\n4 accuracy  binary         0.772 Preprocessor1_Model1\n5 kap       binary         0.492 Preprocessor1_Model1\n6 sens      binary         0.903 Preprocessor1_Model1\n7 spec      binary         0.562 Preprocessor1_Model1\n8 roc_auc   binary         0.789 Preprocessor1_Model1\n\n\nBased on our results we have a roc_auc of 0.7924757 which is generally considered a good performance although we could do better. This means our model has a high ability of finding true positive results than false positives.\nBased on our results, we should also examine variable importance to identify the key features influencing the classification.\n\nlast_fit_rf %&gt;%\n  pluck(\".workflow\", 1) %&gt;%  \n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 10)\n\n\n\n\n\n\n\n\nFrom the model we see the two most important predictors for our models is Sex and Age of the passenger.\nWe now take a look at the confusion matrix for the final model:\n\nlast_fit_rf %&gt;%\n  collect_predictions() |&gt;\n  conf_mat(Survived, .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nWe shall also create an ROC Curve for the final model:\n\nlast_fit_rf |&gt;\n  collect_predictions() |&gt;\n  roc_curve(Survived, .pred_0) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nGiven that the validation and test set performances are similar, we conclude that the Random Forest model with the selected hyperparameters is the best choice for predicting Survival on the Titanic.\n\nrf_predictions &lt;- last_fit_rf %&gt;%\n  collect_predictions()\n\n\n# Impute missing values in the test set\nttest &lt;- ttest %&gt;%\n  group_by(Pclass) %&gt;%\n  mutate(Age = mean(Age, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Age = ifelse(is.na(Age), Age, Age))\n\n# extract fitted workflow\nfinal_workflow &lt;- extract_workflow(last_fit_rf)\n\n# workflow for predictions\nrf_predictions &lt;- predict(final_workflow, ttest) %&gt;%\n  bind_cols(ttest %&gt;% select(PassengerId))\n\n# reorder table\nrf_predictions &lt;- rf_predictions %&gt;%\n  select(PassengerId, everything()) %&gt;% \n  rename(PassengerId = PassengerId, Survived = .pred_class)\n\nTo evaluate our model’s performance on the provided test set, we generated predictions using the final model and submitted them to Kaggle, achieving a Public Score of 0.77751.\n\nThis result, achieved with minimal feature engineering as demonstrated in the tutorial, indicates a somewhat good performance. However, there is potential for further improvement.\nBy incorporating additional feature engineering and exploring more advanced techniques, one could enhance the model’s accuracy.\nFurther feature extraction and refinement are recommended for those looking to achieve even better results for this model.\n\n# submission file\nwrite_csv(rf_predictions, \"submissionfile.csv\")\n\nTo conclude this tutorial, the data used in this project comes from the Kaggle Titanic - Machine Learning from Disaster competition. You can access and download the dataset by visiting the following Kaggle page."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "This was my initial Thesis Research on Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/post-with-code/index.html#thesis-research",
    "href": "posts/post-with-code/index.html#thesis-research",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "This was my initial Thesis Research on Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Thesis/index.html",
    "href": "posts/Thesis/index.html",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Thesis/index.html#thesis-research",
    "href": "posts/Thesis/index.html#thesis-research",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "This was my initial Thesis Research on Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Thesis/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "href": "posts/Thesis/index.html#model-based-clustering-of-longitudinal-high-dimensional-data",
    "title": "Model Based Clustering of High Dimensional Longitudinal Data",
    "section": "",
    "text": "Using Model based Methods for clustering longitudinal high dimensional data, we used mixture models to classify peptides with similar deuterium exchange profiles overtime. The code for the fitting of model can be found here."
  },
  {
    "objectID": "posts/Welcome/index.html",
    "href": "posts/Welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nAfter 6 years of employment, I decided to take a two-year break to pursue my Master’s in Statistics and Data Science. The past few years have been an immersive learning experience, allowing me grow my analytical and statistical skills while diving deep into the health sciences field. But let me take you back to where it all started.\n\nThe Start: Research, Data Analytics, and Business Advisory\nMy professional journey started with me working as a Research Associate at Inachee. Where I worked with a team that conducted business sector analysis, financial appraisals, business plans and feasibility studies. It was here that I developed a passion for translating data into actionable business insights. This role introduced me to data management, as I was responsible for overseeing the curating and development of a cloud-based data bank where the company would market and sale its research assets to customers, and I began to understand the power of data-driven decisions.\nFrom Inachee, I transitioned into EnerGrow, a startup that focuses on asset financing for SMEs in Uganda. Over the years, I held various roles within the company, from Research and Data Analyst to Expansion Manager and ultimately a Research and Development Consultant. I was fortunate to lead the design of a pilot asset financing loan program and manage stakeholder reporting and partnerships on projects financed by organizations like EEP, Doen Foundation, GIZ, MECs Co., IMEU, EnDev, CEI and Rockefeller Foundation. These roles shaped my expertise in managing and analyzing data, building reports, and managing projects. I was able to work across multiple functions, helping the company grow and expand to new regions in Uganda.\n\n\nA New Chapter: My Master’s in Data Science\nDespite the exciting projects I was involved in, I felt the urge to dive deeper into the technical side of data analysis. That’s why I took a two-year break to pursue my Master’s degree in Statistics and Data Science at the University of Hasselt in Belgium. My interest in health sciences was sparked by the growing role of data sciences and statistics in solving some of the most complex biological problems through data. I wanted to be part of that innovation and bring my experience in research and analytics into this cutting-edge field.\n\n\nLooking Forward: Open to New Opportunities\nNow that I’m nearing the end of my Master’s, I am more than ready to apply my expanded knowledge in data science, statistics, and bioinformatics. I’m looking for opportunities that will allow me to continue learning, growing, and making an impact, especially in health sciences or any other sector where data can drive meaningful change. I believe the blend of my past experience with my new expanded knowledge can offer unique perspectives and solutions.\nThis blog will serve as a platform for me to share some of the projects that interest me and what I have worked on, insights on data science, and my journey of navigating this ever-evolving field. Whether you’re here to learn more about data science applications or you’re simply curious about its application, I hope you find something interesting."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Nangosyah Tom Wellard",
    "section": "",
    "text": "![](TomNangosyah_CV_2024_git.pdf){width=100% height=1000px}"
  }
]